import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from transformers import AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig, AutoTokenizer # BitsAndBytesConfig ì¶”ê°€
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training,PeftModel ,TaskType # peft ê´€ë ¨ ëª¨ë“ˆ ì¶”ê°€
import pandas as pd
import shutil
import os
import numpy as np
from torch.nn import CrossEntropyLoss
from torch.utils.data import random_split
import random
import itertools
import gc
import nltk
from rouge_score import rouge_scorer
import jiwer
from sklearn.model_selection import train_test_split
import time
from tqdm import tqdm
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
import math
from pathlib import Path
import csv
from safetensors.torch import load_file

SEED = 42
os.environ["CUDA_VISIBLE_DEVICES"] = "1" 
def set_seeds(seed_value):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)  # ì—¬ëŸ¬ GPU ì‚¬ìš© ì‹œ
        # CUDA ì—°ì‚°ì˜ ê²°ì •ë¡ ì  ì‹¤í–‰ ì„¤ì • (ì„±ëŠ¥ì— ì•½ê°„ ì˜í–¥ ì¤„ ìˆ˜ ìˆìŒ)
        # torch.backends.cudnn.deterministic = True
        # torch.backends.cudnn.benchmark = False

set_seeds(SEED)
print(f"ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‹œë“œê°€ {SEED}ë¡œ ê³ ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

class EEGDataset(Dataset):
    def __init__(self, data_dir="/home/work/skku/hyo/hyo/dataset/word-sentence.parquet"):
        df = pd.read_parquet(data_dir)
        raw = df["eeg"].tolist()
        print(f"[1] ë¡œë“œëœ ìƒ˜í”Œ ê°œìˆ˜: {len(raw)}")

        word_arrays = []
        for idx, x in enumerate(raw):
            # ë¬¸ì¥ EEGë§Œ ìˆëŠ” ê²½ìš°ì—ë„ ë§ˆì§€ë§‰ ìš”ì†Œ í¬í•¨
            vecs = x[:-1] if len(x) > 1 else x
            print(f"  ìƒ˜í”Œ {idx} - ì›ë³¸ ê¸¸ì´: {len(x)}, ì‚¬ìš©í•  word-level EEG ê°œìˆ˜: {len(vecs)}")

            words = []
            for v in vecs:
                arr = np.asarray(v, dtype=np.float32).ravel()
                assert arr.ndim == 1, f"ë²¡í„° ì°¨ì› ì˜¤ë¥˜: {arr.shape}"
                words.append(arr)
            stacked = np.stack(words, axis=0)  # (Ki, 840)
            print(f"    â†’ ìŠ¤íƒ í›„ shape: {stacked.shape}")
            word_arrays.append(stacked)

        # ì „ì²´ ë‹¨ì–´ ìˆ˜ ì§‘ê³„
        all_words = np.vstack(word_arrays)   # (ì´_ë‹¨ì–´_ìˆ˜, 840)
        print(f"[2] all_words ìŠ¤íƒ shape: {all_words.shape}")

        all_words = np.nan_to_num(
            all_words, nan=0.0, posinf=0.0, neginf=0.0
        )
        mu  = all_words.mean(axis=0, keepdims=True)    # (1, 840)
        std = all_words.std(axis=0,  keepdims=True) + 1e-8
        print(f"[3] mu shape: {mu.shape}, std shape: {std.shape}")

        # ì •ê·œí™” ë° Torch ë³€í™˜
        self.eeg_seqs = []
        for idx, arr in enumerate(word_arrays):
            normed = (arr - mu) / std                  # (Ki, 840)
            print(f"  ìƒ˜í”Œ {idx} ì •ê·œí™” í›„ shape: {normed.shape}")
            self.eeg_seqs.append(torch.from_numpy(normed))

        # ë§ˆì§€ë§‰ìœ¼ë¡œ í…ìŠ¤íŠ¸
        self.texts = [
            toks[-1] if isinstance(toks, (list, np.ndarray)) else toks
            for toks in df["text"].tolist()
        ]
        print(f"[4] text ìƒ˜í”Œ ê°œìˆ˜: {len(self.texts)}")

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.eeg_seqs[idx], self.texts[idx]

class ConvEEGEncoder(nn.Module):
    """
    840-dim ë²¡í„°ë¥¼ 1Ã—840 ì‹œí€€ìŠ¤ë¡œ ë³´ê³  Conv1D ë‘ ì¸µìœ¼ë¡œ ì ì¬í‘œí˜„ ìƒì„±
    ì¶œë ¥ì€ (B, latent_dim)
    """
    def __init__(self, input_dim=840, latent_dim=128, hidden=256):
        super().__init__()
        self.conv_stack = nn.Sequential(
            nn.Conv1d(1, hidden, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv1d(hidden, latent_dim, kernel_size=3, padding=1), nn.ReLU()
        )
        self.pool = nn.AdaptiveAvgPool1d(1)   # ê¸¸ì´ 840 â†’ 1 ë¡œ ì••ì¶•

    def forward(self, x):           # x: (B, feat)
        x = x.unsqueeze(1)          # (B, 1, 840)
        z = self.conv_stack(x)      # (B, latent_dim, 840)
        z = self.pool(z).squeeze(-1)  # (B, latent_dim)
        return z

class RVQ(nn.Module):
    def __init__(self, num_quantizers, num_embeddings, embedding_dim, commitment_cost=0.25):
        super().__init__()
        self.num_quantizers = num_quantizers # ì½”ë“œë¶ì˜ ê°œìˆ˜ (n_q)
        self.num_embeddings = num_embeddings # ê° ì½”ë“œë¶ ë‚´ ì„ë² ë”©(ì½”ë“œì›Œë“œ) ê°œìˆ˜ (n_emb, ì–´íœ˜ í¬ê¸°)
        self.embedding_dim = embedding_dim   # ê° ì„ë² ë”©ì˜ ì°¨ì› (D, latent_dimê³¼ ë™ì¼)
        self.commitment_cost = commitment_cost # VQ ì†ì‹¤ ê³„ì‚° ì‹œ ì‚¬ìš©ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°

        # num_quantizers ê°œìˆ˜ë§Œí¼ì˜ ì½”ë“œë¶(nn.Embedding ë ˆì´ì–´)ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì§
        self.codebooks = nn.ModuleList([
            nn.Embedding(self.num_embeddings, self.embedding_dim) for _ in range(self.num_quantizers)
        ])
        # ì½”ë“œë¶ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜í–‰)
        for i, codebook in enumerate(self.codebooks):
            nn.init.uniform_(codebook.weight, -1.0 / self.num_embeddings, 1.0 / self.num_embeddings)

    def forward(self, z_e): # ì…ë ¥ z_eì˜ ëª¨ì–‘: (B, L, D), ì—¬ê¸°ì„œ L=1, D=embedding_dim
        B, L, D = z_e.shape
        z_e_flat = z_e.reshape(-1, D) # (B*L, D) ëª¨ì–‘ìœ¼ë¡œ í¼ì¹¨ (ì—¬ê¸°ì„œëŠ” (B, D)ì™€ ë™ì¼)

        all_quantized_stages = [] # ê° ì½”ë“œë¶ì—ì„œ ì–‘ìí™”ëœ ë²¡í„°ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        all_indices = []          # ê° ì½”ë“œë¶ì—ì„œ ì„ íƒëœ ì¸ë±ìŠ¤ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        residual = z_e_flat       # ì²« ë²ˆì§¸ ì½”ë“œë¶ì— ì…ë ¥ë  ì”ì°¨ (ì´ˆê¸°ì—ëŠ” z_e_flat ì „ì²´)

        # num_quantizers ë§Œí¼ ë°˜ë³µ (ê° ì½”ë“œë¶ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬)
        for i in range(self.num_quantizers):
            codebook = self.codebooks[i] # í˜„ì¬ ì‚¬ìš©í•  ì½”ë“œë¶

            # í˜„ì¬ ì”ì°¨(residual)ì™€ í˜„ì¬ ì½”ë“œë¶ì˜ ëª¨ë“  ì„ë² ë”© ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì œê³± ê³„ì‚°
            # distances ëª¨ì–‘: (B*L, num_embeddings)
            distances = torch.sum(residual**2, dim=1, keepdim=True) \
                        - 2 * torch.matmul(residual, codebook.weight.t()) \
                        + torch.sum(codebook.weight**2, dim=1, keepdim=True).t()

            # ê°€ì¥ ê°€ê¹Œìš´ ì„ë² ë”©ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            # current_indices ëª¨ì–‘: (B*L)
            current_indices = torch.argmin(distances, dim=1)
            all_indices.append(current_indices) # í˜„ì¬ ì½”ë“œë¶ì˜ ì¸ë±ìŠ¤ ì €ì¥

            # ì„ íƒëœ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–‘ìí™”ëœ ë²¡í„°(ì½”ë“œì›Œë“œ) ê°€ì ¸ì˜¤ê¸°
            # quantized_vector ëª¨ì–‘: (B*L, D)
            quantized_vector = codebook(current_indices)
            # ì›ë˜ ëª¨ì–‘ (B, L, D)ë¡œ ë³µì›í•˜ì—¬ ì €ì¥ (ì—¬ê¸°ì„œëŠ” (B, 1, D))
            all_quantized_stages.append(quantized_vector.reshape(B, L, D))

            # ë‹¤ìŒ ì½”ë“œë¶ìœ¼ë¡œ ë„˜ê¸¸ ì”ì°¨ ê³„ì‚°
            # ì¤‘ìš”: quantized_vectorì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ì„ ëŠê¸° ìœ„í•´ .detach() ì‚¬ìš©
            residual = residual - quantized_vector.detach()

        # ëª¨ë“  ì½”ë“œë¶ì—ì„œ ë‚˜ì˜¨ ì–‘ìí™”ëœ ë²¡í„°ë“¤ì„ í•©ì‚° (EEGTran ë…¼ë¬¸ Figure 2 ì°¸ì¡°)
        # final_quantized_output ëª¨ì–‘: (B, L, D)
        final_quantized_output = torch.stack(all_quantized_stages, dim=0).sum(dim=0)

        # ìˆ˜ì§‘ëœ ì¸ë±ìŠ¤ë“¤ì„ (B, L, num_quantizers) í˜•íƒœë¡œ ìŒ“ìŒ
        # stacked_indices ëª¨ì–‘: (B, L, n_q) (ì—¬ê¸°ì„œëŠ” (B, 1, n_q))
        stacked_indices = torch.stack(all_indices, dim=1).reshape(B, L, self.num_quantizers)

        # ìµœì¢… ë°˜í™˜ê°’: í•©ì‚°ëœ ì–‘ìí™” ë²¡í„°, ìŒ“ì¸ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤, VQ ì†ì‹¤
        # RVQTokenizerì˜ forwardì—ì„œëŠ” ì´ ì¤‘ ì²« ë‘ ê°œë¥¼ zq, indicesë¡œ ë°›ê²Œ ë©ë‹ˆë‹¤.
        return final_quantized_output, stacked_indices

class RVQTokenizer(nn.Module):
    def __init__(self,
                 feat=840,
                 latent=128,  # 1024->2048
                 n_q=12,
                 n_emb=512,
                 hidden=256,
                 TOKENIZER_CHECKPOINT_PATH = "/home/work/skku/hyo/hyo/model/rvq_best_model_word_12_512.pt"
                 ):
        super().__init__()
        self.n_q = n_q
        self.n_emb = n_emb
        # ì‹¤ì œ ConvEEGEncoderì™€ RVQ ëª¨ë“ˆì´ ì—¬ê¸°ì— ì™€ì•¼ í•¨
        self.enc = ConvEEGEncoder(feat, latent, hidden)
        self.rvq = RVQ(num_quantizers=n_q, num_embeddings=n_emb, embedding_dim=latent)

        checkpoint = torch.load(TOKENIZER_CHECKPOINT_PATH, map_location="cpu")
        self.enc.load_state_dict(checkpoint["encoder"])
        for i, cb_weight_tensor in enumerate(checkpoint["codebooks"]):
            self.rvq.codebooks[i].weight.data = cb_weight_tensor

    @torch.no_grad()
    def forward(self, x): # x: (B, 840)
        z = self.enc(x)
        quantized_vector, token_indices = self.rvq(z.unsqueeze(1)) # vq_lossëŠ” ë¬´ì‹œ
        zq = quantized_vector
        indices = token_indices # ëª¨ì–‘ (B, 1, n_q)
        # ë§Œì•½ LLaDA ì…ë ¥ìš©ìœ¼ë¡œ (B, n_q) ëª¨ì–‘ì˜ ì¸ë±ìŠ¤ë¥¼ ì›í•œë‹¤ë©´ squeeze(1) í•„ìš”
        # return zq, indices.squeeze(1)
        return zq, indices # í˜„ì¬ pasted_content.txtì˜ ì£¼ì„ê³¼ ë§ì¶”ë ¤ë©´ ì´ëŒ€ë¡œ

class UnifiedEEGTextTokenizer:
    def __init__(self,
                rvq_tokenizer_instance,
                llada_text_tokenizer_instance,
                max_seq_length,
                v_text_original,
                eeg_token_length,
                ):

        self.rvq_tokenizer = rvq_tokenizer_instance
        self.llada_text_tokenizer = llada_text_tokenizer_instance
        self.max_seq_length = max_seq_length
        self.v_text_original = v_text_original
        self.eeg_token_length = eeg_token_length


        self.bos_token_id = torch.tensor([self.llada_text_tokenizer.bos_token_id], dtype=torch.long, device=config.system.DEVICE)
        self.eos_token_id = torch.tensor([self.llada_text_tokenizer.eos_token_id], dtype=torch.long, device=config.system.DEVICE)
        self.pad_token_id = self.llada_text_tokenizer.pad_token_id if self.llada_text_tokenizer.pad_token_id is not None else self.llada_text_tokenizer.eos_token_id

        self.user_prompt_intro_ids = self.llada_text_tokenizer.encode(
                "<start_id>user<end_id>\n",
                add_special_tokens=False,
                return_tensors="pt"
            ).squeeze(0).to(config.system.DEVICE)

        self.assistant_prompt_intro_ids = self.llada_text_tokenizer.encode(
                "<eot_id><start_id>assistant<end_id>\n",
                add_special_tokens=False,
                return_tensors="pt"
            ).squeeze(0).to(config.system.DEVICE)

        print(f"Unified Tokenizer Initialized:")
        print(f"  BOS ID: {self.bos_token_id.item()}")
        print(f"  EOS ID: {self.eos_token_id.item()}")
        print(f"  PAD ID: {self.pad_token_id}")
        print(f"  User Prompt Intro IDs ({len(self.user_prompt_intro_ids)} tokens): {self.user_prompt_intro_ids.tolist()}")
        print(f"  Assistant Prompt Intro IDs ({len(self.assistant_prompt_intro_ids)} tokens): {self.assistant_prompt_intro_ids.tolist()}")

    def process_single_sample(self, eeg_seq, assistant_response_text):
        # eeg_seq: (N_words, 840)
        device = config.system.DEVICE

        # 1) ê° word EEG ë²¡í„° â†’ RVQTokenizer â†’ (1,1,n_q) indices â†’ (n_q,) tensor
        local_indices = []
        with torch.no_grad():
            for w in eeg_seq.to(device):                          # w: (840,)
                _, idx = self.rvq_tokenizer(w.unsqueeze(0))       # idx.shape = (1,1,n_q)
                local_indices.append(idx.squeeze(0).squeeze(0))    # (n_q,)
        # ìµœì¢… shape: (N_words * n_q,)
        local_indices = torch.cat(local_indices, dim=0)

        # 2) global token IDs: ê¸°ì¡´ text vocab_size offset
        global_eeg_ids = (local_indices + self.v_text_original).to(device)  # LongTensor

        # 3) prompt ê¸¸ì´ ê³ ì • í† í° ìˆ˜ ê³„ì‚°
        fixed_len = (
            len(self.bos_token_id)
          + len(self.user_prompt_intro_ids)
          + len(self.assistant_prompt_intro_ids)
          + len(self.eos_token_id)
        )
        # ë‚¨ëŠ” ê³µê°„
        max_resp = self.max_seq_length - fixed_len - global_eeg_ids.numel()
        if max_resp < 1:
            # EEG í† í°ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ…ë‹ˆë‹¤
            global_eeg_ids = global_eeg_ids[: self.max_seq_length - fixed_len - 1]
            max_resp = 1

        # 4) ì–´ì‹œìŠ¤í„´íŠ¸ í…ìŠ¤íŠ¸ í† í°í™”
        tok = self.llada_text_tokenizer(
            assistant_response_text,
            max_length=max_resp,
            truncation=True,
            add_special_tokens=False,
            return_tensors="pt"
        )
        assistant_ids = tok.input_ids.squeeze(0).to(device)

        # 5) input_ids, labels, attention_mask êµ¬ì„±
        input_ids = torch.cat([
            self.bos_token_id,
            self.user_prompt_intro_ids,
            global_eeg_ids,
            self.assistant_prompt_intro_ids,
            assistant_ids,
            self.eos_token_id
        ], dim=0)

        prompt_len = (
            len(self.bos_token_id)
          + len(self.user_prompt_intro_ids)
          + global_eeg_ids.numel()
          + len(self.assistant_prompt_intro_ids)
        )

        # labels: prompt ìœ„ì¹˜ëŠ” -100
        labels = input_ids.clone()
        labels[:prompt_len] = -100

        # padding / truncate
        cur_len = input_ids.numel()
        if cur_len < self.max_seq_length:
            pad_len = self.max_seq_length - cur_len
            pad = torch.full((pad_len,), self.pad_token_id, dtype=torch.long, device=device)
            input_ids    = torch.cat([input_ids, pad], dim=0)
            labels       = torch.cat([labels, torch.full((pad_len,), -100, dtype=torch.long, device=device)], dim=0)
            attn_mask    = torch.cat([torch.ones(cur_len, dtype=torch.long, device=device),
                                      torch.zeros(pad_len, dtype=torch.long, device=device)], dim=0)
        else:
            input_ids    = input_ids[:self.max_seq_length]
            labels       = labels[:self.max_seq_length]
            attn_mask    = torch.ones(self.max_seq_length, dtype=torch.long, device=device)

        return {
            "input_ids": input_ids,
            "attention_mask": attn_mask,
            "labels": labels,
            "prompt_lengths": torch.tensor(prompt_len, dtype=torch.long, device=device)
        }

    def build_chat_template_prompt(self, eeg_seq):
        device = config.system.DEVICE
        # â‘  RVQ â†’ indices
        local_indices = []
        with torch.no_grad():
            for w in eeg_seq.to(device):
                _, idx = self.rvq_tokenizer(w.unsqueeze(0))
                local_indices.append(idx.squeeze(0).squeeze(0))
        global_eeg_ids = (torch.cat(local_indices, dim=0) + self.v_text_original).to(device)

        input_ids = torch.cat([
            self.bos_token_id,
            self.user_prompt_intro_ids,
            global_eeg_ids,
            self.assistant_prompt_intro_ids
        ], dim=0)

        return {
            "input_ids": input_ids.unsqueeze(0),
            "attention_mask": torch.ones_like(input_ids).unsqueeze(0),
            "prompt_len": input_ids.numel()
        }

class DataCollatorForEEGTextSFT:
    def __init__(self, unified_tokenizer_instance):
        self.unified_tokenizer = unified_tokenizer_instance

    def __call__(self, batch_of_samples):
        processed_samples = []
        for eeg_tensor, assistant_response_text in batch_of_samples:
            if eeg_tensor is None or assistant_response_text is None:
                continue
            processed_samples.append(self.unified_tokenizer.process_single_sample(eeg_tensor, assistant_response_text))

        if not processed_samples:
            # print("Warning: Collator received no valid samples to batch.")
            # ë¹ˆ í…ì„œë¥¼ ë°˜í™˜í•˜ê±°ë‚˜ Noneì„ ë°˜í™˜í•˜ì—¬ í•™ìŠµ ë£¨í”„ì—ì„œ ì²˜ë¦¬
            return None

        batched_input_ids = torch.stack([s["input_ids"] for s in processed_samples])
        batched_attention_mask = torch.stack([s["attention_mask"] for s in processed_samples])
        batched_labels = torch.stack([s["labels"] for s in processed_samples])
        batched_prompt_lengths = torch.stack([s["prompt_lengths"] for s in processed_samples])

        return {
            "input_ids": batched_input_ids,
            "attention_mask": batched_attention_mask,
            "labels": batched_labels,
            "prompt_lengths": batched_prompt_lengths
        }

def forward_process(input_ids, llada_mask_token_id=126336, eps=1e-3):
    # llada_mask_token_idëŠ” LLaDAì˜ [MASK] í† í° IDì…ë‹ˆë‹¤.
    b, l = input_ids.shape
    t = torch.rand(b, device=input_ids.device)
    p_mask = (1 - eps) * t + eps # ê° ìƒ˜í”Œë³„ ë§ˆìŠ¤í‚¹ í™•ë¥  (ìŠ¤ì¹¼ë¼)
    p_mask_expanded = p_mask[:, None].repeat(1, l) # (B, L) í˜•íƒœë¡œ í™•ì¥

    # ê° í† í° ìœ„ì¹˜ë³„ë¡œ ë§ˆìŠ¤í‚¹ ì—¬ë¶€ ê²°ì •
    masked_indices_bool = torch.rand((b, l), device=input_ids.device) < p_mask_expanded

    noisy_batch = torch.where(masked_indices_bool, llada_mask_token_id, input_ids)
    # p_mask_expandedëŠ” (B,L) ëª¨ì–‘, ê° í† í° ìœ„ì¹˜ì— ì ìš©ëœ ë§ˆìŠ¤í‚¹ í™•ë¥  p(t)ë¥¼ ê°€ì§
    return noisy_batch, masked_indices_bool, p_mask_expanded

class EEGTextLLaDA(nn.Module):
    def __init__(self,
                 llada_model_name="GSAI-ML/LLaDA-8B-Base",
                 rvq_n_emb=512,
                 use_qlora=True,
                 qlora_r=16, qlora_alpha=32, qlora_dropout=0.05,
                 qlora_target_modules=("q_proj", "v_proj"),
                 pretrained_adapter_path="/home/work/skku/hyo/hyo/grid_search_results_word/overall_best_model/qlora_adapter"):
        super().__init__()

        # ---------- â‘  4-bit LLaDA ë¡œë“œ ----------
        bnb_cfg = BitsAndBytesConfig(
            load_in_4bit=True, bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True
        ) if use_qlora else None

        self.llada_model = AutoModelForCausalLM.from_pretrained(
            llada_model_name,
            quantization_config=bnb_cfg,
            torch_dtype="auto",
            trust_remote_code=True,
        )

        # ---------- â‘¡ ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ ----------
        v_text = self.llada_model.config.vocab_size
        new_vocab = v_text + rvq_n_emb + 1          # +1 = EEG-MASK
        self.llada_model.resize_token_embeddings(new_vocab)
        self.global_eeg_mask_token_id = v_text + rvq_n_emb   # for tokenizer

        # ---------- â‘¢ prepare for k-bit ----------
        if use_qlora:
            self.llada_model = prepare_model_for_kbit_training(
                self.llada_model, use_gradient_checkpointing=False
            )

            # ---------- â‘£ LoRA attach / load  ----------
            if pretrained_adapter_path:
                print("Loading LoRA:", pretrained_adapter_path)
                self.llada_model = PeftModel.from_pretrained(
                    self.llada_model, pretrained_adapter_path, is_trainable=True
                )
            else:
                lora_cfg = LoraConfig(
                    r=qlora_r, lora_alpha=qlora_alpha, lora_dropout=qlora_dropout,
                    target_modules=list(qlora_target_modules),
                    bias="none", task_type=TaskType.CAUSAL_LM,
                )
                self.llada_model = get_peft_model(self.llada_model, lora_cfg)

            # ---------- â‘¤  ì„ë² ë”© & LM-head un-freeze ----------
            inp_emb = self.llada_model.base_model.model.model.transformer.wte      # wte
            inp_emb.weight.requires_grad_(True)

            for n, p in self.llada_model.named_parameters():
                if n.endswith("transformer.ff_out.weight"):
                    p.requires_grad = True                       # lm_head

            # (ì„ íƒ) ì–´ëŒ‘í„° ì €ì¥ ì‹œ ê°™ì´ ë³´ì¡´
            self.llada_model.modules_to_save = ["embed_tokens", "lm_head"]

            print("\nTrainable flags (should be True)")
            for n, p in self.llada_model.named_parameters():
                if "wte" in n or n.endswith("ff_out.weight") or "lora_" in n:
                    print(f"{n:65} {p.requires_grad}")


            # (1) LM-head í•˜ë‚˜ë§Œ un-freeze
            for n, p in self.llada_model.named_parameters():
                if n.endswith("transformer.ff_out.weight"):   # <- ë§¨ ë§ˆì§€ë§‰ ff_out í•˜ë‚˜ë§Œ ë§¤ì¹­
                    p.requires_grad_(True)

            # (2) ë””ë²„ê·¸/ê²€ì¦ìš© assert ë„ ë™ì¼í•˜ê²Œ LM-head í•˜ë‚˜ë§Œ í™•ì¸
            for n, p in self.llada_model.named_parameters():
                if n in (
                    "base_model.model.model.transformer.wte.weight",
                    "base_model.model.model.transformer.ff_out.weight",
                ):
                    assert p.requires_grad, f"{n} is frozen!"


    # ---------- forward ----------
    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.llada_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            return_dict=True
        )
        return outputs

# ----------------- 2) train_loop -----------------
def train_loop(
    model, train_dataloader, optimizer, device,
    current_epoch, total_epochs,
    llada_mask_token_id, forward_process_fn,
    gradient_accumulation_steps=1, max_grad_norm=1.0,
    scheduler=None, log_interval=10
):
    model.train()
    total_epoch_loss = 0
    num_batches      = 0

    bar = tqdm(train_dataloader,
               desc=f"Epoch {current_epoch}/{total_epochs} [Train]",
               leave=False)

    for step, batch in enumerate(bar):
        if batch is None:
            continue

        input_ids  = batch["input_ids"].to(device)        # (B,S)
        attn_mask  = batch["attention_mask"].to(device)
        pr_lengths = batch["prompt_lengths"].to(device)   # (B,)

        B, S = input_ids.shape

        # 1) forward-process
        noisy, mask_bool, p_mask = forward_process_fn(
            input_ids, llada_mask_token_id)

        # 2) prompt/response mask
        prompt_bool = torch.arange(S, device=device)[None, :] < pr_lengths[:, None]
        resp_bool   = ~prompt_bool

        # 2-1) prompt í† í° ë³µì›
        noisy = torch.where(prompt_bool, input_ids, noisy)
        loss_mask_bool = mask_bool & resp_bool        # ë‹µë³€+ë§ˆìŠ¤í¬

        # ---------- forward ----------              <-- â˜… forward ë¨¼ì €!
        outputs = model(noisy, attention_mask=attn_mask)
        logits  = outputs.logits                     # (B,S,V)

        # ---------- loss  --------------------------
        loss = calculate_llada_custom_loss(
                logits, input_ids,
                loss_mask_bool, p_mask,           # â† p_mask ì „ë‹¬
                device)

        # 5) Grad-accum
        loss_scaled = loss / gradient_accumulation_steps
        loss_scaled.backward()

        total_epoch_loss += loss.item()
        num_batches      += 1

        if (step + 1) % gradient_accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step(); optimizer.zero_grad()
            if scheduler: scheduler.step()

        if (step + 1) % log_interval == 0:
            bar.set_postfix({'train_loss': f"{loss.item():.4f}"})

    return total_epoch_loss / max(num_batches, 1)

# ----------------- 3) validation_loop -----------------
def validation_loop(model, val_dataloader, device,
                    forward_process_fn, llada_mask_token_id):

    model.eval()
    tot_loss, n_batches = 0.0, 0

    with torch.no_grad():
        for batch in tqdm(val_dataloader, desc="Val", leave=False):
            if batch is None:
                continue

            input_ids      = batch["input_ids"].to(device)      # (B,S)
            attn_mask      = batch["attention_mask"].to(device)
            prompt_lengths = batch["prompt_lengths"].to(device)  # (B,)

            B, S = input_ids.shape

            # 1) forward-process
            noisy, mask_bool, p_mask = forward_process_fn(
                input_ids, llada_mask_token_id)

            prm_bool  = torch.arange(S, device=device)[None, :] < prompt_lengths[:, None]
            rsp_bool  = ~prm_bool
            noisy     = torch.where(prm_bool, input_ids, noisy)
            loss_bool = mask_bool & rsp_bool

            outputs = model(noisy, attention_mask=attn_mask)
            logits  = outputs.logits

            loss = calculate_llada_custom_loss(
                    logits, input_ids,
                    loss_bool, p_mask,                # â† p_mask ì „ë‹¬
                    device)

            tot_loss += loss.item()
            n_batches += 1

    return tot_loss / max(n_batches, 1)

def quick_generate(model, tokenizer_wrap, eeg_tensor, gen_len=128,
                   steps=128, block=32, temp=0, cfg=0):

    # â‘  EEGâ†’prompt í…œí”Œë¦¿
    prompt_pack = tokenizer_wrap.build_chat_template_prompt(eeg_tensor)
    prompt_ids  = prompt_pack["input_ids"].to(model.device)      # (1,T)
    prompt_len  = prompt_pack["prompt_len"]

    # â‘¡ LLaDA diffusion sample
    out_ids = generate(model,                       # PeftModel ìì²´
                       prompt=prompt_ids,# (T,)
                       steps=steps, gen_length=gen_len,
                       block_length=block,
                       temperature=temp, cfg_scale=cfg)

    # â‘¢ ê²°ê³¼ ë””ì½”ë”© (prompt ì´í›„ë§Œ)
    txt = tokenizer_wrap.llada_text_tokenizer.decode(
            out_ids[0, prompt_len:], skip_special_tokens=True)
    return txt.strip()

def add_gumbel_noise(logits, temperature):
    '''
    The Gumbel max is a method for sampling categorical distributions.
    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.
    Thus, we use float64.
    '''
    if temperature == 0:
        return logits
    logits = logits.to(torch.float64)
    noise = torch.rand_like(logits, dtype=torch.float64)
    gumbel_noise = (- torch.log(noise)) ** temperature
    return logits.exp() / gumbel_noise


def get_num_transfer_tokens(mask_index, steps):
    '''
    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.
    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),
    the expected number of tokens transitioned at each step should be consistent.

    This function is designed to precompute the number of tokens that need to be transitioned at each step.
    '''
    mask_num = mask_index.sum(dim=1, keepdim=True)

    base = mask_num // steps
    remainder = mask_num % steps

    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base

    for i in range(mask_num.size(0)):
        num_transfer_tokens[i, :remainder[i]] += 1

    return num_transfer_tokens


@ torch.no_grad()
def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,
             cfg_scale=0., remasking='low_confidence', mask_id=126336):
    '''
    Args:
        model: Mask predictor.
        prompt: A tensor of shape (1, L).
        steps: Sampling steps, less than or equal to gen_length.
        gen_length: Generated answer length.
        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
        temperature: Categorical distribution sampling temperature.
        cfg_scale: Unsupervised classifier-free guidance scale.
        remasking: Remasking strategy. 'low_confidence' or 'random'.
        mask_id: The toke id of [MASK] is 126336.
    '''
    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    prompt_index = (x != mask_id)

    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length

    assert steps % num_blocks == 0
    steps = steps // num_blocks

    for num_block in range(num_blocks):
        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)
        for i in range(steps):
            mask_index = (x == mask_id)
            if cfg_scale > 0.:
                un_x = x.clone()
                un_x[prompt_index] = mask_id
                x_ = torch.cat([x, un_x], dim=0)
                logits = model(x_).logits
                logits, un_logits = torch.chunk(logits, 2, dim=0)
                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)
            else:
                logits = model(x).logits

            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)
            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l

            if remasking == 'low_confidence':
                p = F.softmax(logits.to(torch.float64), dim=-1)
                x0_p = torch.squeeze(
                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l
            elif remasking == 'random':
                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)
            else:
                raise NotImplementedError(remasking)

            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf

            x0 = torch.where(mask_index, x0, x)
            confidence = torch.where(mask_index, x0_p, -np.inf)

            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)
            for j in range(confidence.shape[0]):
                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])
                transfer_index[j, select_index] = True
            x[transfer_index] = x0[transfer_index]

    return x

def calculate_llada_custom_loss(
        logits, input_ids, final_mask_bool,
        p_mask_expanded,              # â† ë‹¤ì‹œ ì¸ìë¡œ ë°›ê¸°
        device):

    logits_masked = logits[final_mask_bool]          # (N,V)
    labels_masked = input_ids[final_mask_bool]
    pvals_masked  = p_mask_expanded[final_mask_bool] # (N,)

    # (1) í† í°ë³„ CE
    loss_tok = F.cross_entropy(
        logits_masked, labels_masked,
        reduction="none"
    )

    # (2) ë…¼ë¬¸ì²˜ëŸ¼ 1/p ê°€ì¤‘
    weighted = loss_tok / pvals_masked.clamp_min(1e-8)

    # (3) ìƒ˜í”Œ-í‰ê·  â†’ ë°°ì¹˜-í‰ê· 
    sample_ids = torch.nonzero(final_mask_bool, as_tuple=False)[:,0]
    B = logits.size(0)
    sum_per = torch.zeros(B, device=device).index_add_(0, sample_ids, weighted)
    cnt_per = torch.zeros(B, device=device).index_add_(0, sample_ids,
                                                       torch.ones_like(weighted))
    return (sum_per / cnt_per.clamp_min(1)).mean()

def save_embedding_and_lm_head(peft_model: PeftModel, save_path: Path):
    save_path.mkdir(parents=True, exist_ok=True)

    # â‘  ì…ë ¥ ì„ë² ë”©
    torch.save(
        peft_model.get_input_embeddings().state_dict(),
        save_path / "wte.pth"
    )

    # â‘¡ LM-head
    llada_core = peft_model.base_model.model.model       # LLaDAModel
    lm_head = llada_core.transformer.ff_out              # â† ë™ì¼ ê²½ë¡œ
    torch.save(lm_head.state_dict(), save_path / "lm_head.pth")

class SystemConfig(BaseModel):
    SEED: int = 42
    DEVICE: str = "cuda" if torch.cuda.is_available() else "cpu"
    NUM_WORKERS: int = 0

class PathsConfig(BaseModel):
    DATASET_PATH: str = "/home/work/skku/hyo/hyo/dataset/word-sentence.parquet"
    TOKENIZER_CHECKPOINT_PATH: str = "/home/work/skku/hyo/hyo/model/rvq_best_model_word_12_512.pt"
    MODEL_SAVE_DIR: str = "./saved_models"
    BEST_MODEL_FILENAME: str = "eeg_llada_sft_best_model.pth"
    # LLADA_LOSS_FUNCTION_PATH: str = "/home/ubuntu/llada_loss_function.py" # í•„ìš”ì‹œ ì¶”ê°€
    # MODIFIED_TRAINING_LOOPS_PATH: str = "/home/ubuntu/modified_training_loops.py" # í•„ìš”ì‹œ ì¶”ê°€

class EEGEncoderConfig(BaseModel):
    INPUT_DIM: int = 840
    LATENT_DIM: int = 128 # RVQì˜ embedding_dimê³¼ ì¼ì¹˜í•´ì•¼ í•¨
    HIDDEN_DIM: int = 256

class RVQConfig(BaseModel):
    NUM_QUANTIZERS: int = 12 # RVQTokenizerì˜ n_q, UnifiedEEGTextTokenizerì˜ eeg_token_lengthì™€ ì¼ì¹˜
    NUM_EMBEDDINGS: int = 512 # RVQTokenizerì˜ n_emb
    EMBEDDING_DIM: int = 128 # EEGEncoderConfigì˜ LATENT_DIMê³¼ ì¼ì¹˜
    COMMITMENT_COST: float = 0.25

class TokenizerConfig(BaseModel):
    # RVQTokenizer ë‚´ë¶€ íŒŒë¼ë¯¸í„° (EEGEncoderConfig, RVQConfig ê°’ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
    # UnifiedEEGTextTokenizer íŒŒë¼ë¯¸í„°
    MAX_SEQ_LENGTH: int = 1024 # LLaDA ëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê³ ë ¤
    V_TEXT_ORIGINAL: int = 32000 # LLaMA í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ í¬ê¸° (LLaDA-8B ê¸°ì¤€)
    # EEG_TOKEN_LENGTH: int = 12 # RVQConfig.NUM_QUANTIZERS ì™€ ë™ì¼
    LLM_MODEL_NAME: str = "GSAI-ML/LLaDA-8B-Base" # LLM í† í¬ë‚˜ì´ì € ë¡œë“œìš©

class ModelConfig(BaseModel):
    LLM_MODEL_NAME: str = "GSAI-ML/LLaDA-8B-Base"
    USE_QLORA: bool = True
    LORA_R: int = 16
    LORA_ALPHA: int = 32
    LORA_DROPOUT: float = 0.05
    LORA_BIAS: str = "none"
    # LLADA_MASK_TOKEN_ID: Optional[int] = None # ë™ì ìœ¼ë¡œ ì„¤ì •ë  ìˆ˜ ìˆìŒ (ì–´íœ˜í¬ê¸° + 1)

class TrainingConfig(BaseModel):
    BATCH_SIZE: int = 4 # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ
    NUM_EPOCHS: int = 10
    START_EPOCH: int = 0
    LEARNING_RATE: float = 1e-4
    GRADIENT_ACCUMULATION_STEPS: int = 4 # BATCH_SIZE * GRAD_ACCUM = ì‹¤ì œ ë°°ì¹˜ í¬ê¸°
    MAX_GRAD_NORM: float = 1.0
    # SCHEDULER: Optional[str] = None # ì˜ˆ: "StepLR"
    TRAIN_LOG_INTERVAL: int = 1
    PATIENCE_EARLY_STOPPING: int = 3 # 0ì´ë©´ ë¹„í™œì„±í™”

class GenerationConfig(BaseModel):
    RUN_TEST_LOOP_EACH_EPOCH: bool = False
    USE_LLADA_SAMPLING_FOR_GENERATION: bool = True
    MAX_GEN_TOKENS: int = 64
    NUM_SAMPLING_STEPS_GEN: int = 10
    REMASKING_STRATEGY_GEN: str = "low_confidence"
    REMASKING_RATIO_GEN: float = 0.25
    TEMPERATURE_GEN: float = 0.7
    TOP_K_GEN: int = 50
    TOP_P_GEN: float = 0.9
    HF_NUM_BEAMS_GEN: int = 1
    # HF_MAX_LENGTH_GEN: Optional[int] = None # ë™ì ìœ¼ë¡œ ì„¤ì • (ì…ë ¥ê¸¸ì´ + MAX_GEN_TOKENS)

class ExperimentConfig(BaseModel):
    system: SystemConfig = Field(default_factory=SystemConfig)
    paths: PathsConfig = Field(default_factory=PathsConfig)
    eeg_encoder: EEGEncoderConfig = Field(default_factory=EEGEncoderConfig)
    rvq: RVQConfig = Field(default_factory=RVQConfig)
    tokenizer: TokenizerConfig = Field(default_factory=TokenizerConfig)
    model: ModelConfig = Field(default_factory=ModelConfig)
    training: TrainingConfig = Field(default_factory=TrainingConfig)
    generation: GenerationConfig = Field(default_factory=GenerationConfig)

    # LLADA_MASK_TOKEN_IDëŠ” ë™ì ìœ¼ë¡œ ì„¤ì •ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ˆê¸°í™” í›„ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥
    # ì˜ˆ: config.model.LLADA_MASK_TOKEN_ID = tokenizer.llada_text_tokenizer.vocab_size + 1

config = ExperimentConfig()

llada_txt_tokenizer = AutoTokenizer.from_pretrained(config.model.LLM_MODEL_NAME)
rvq_eeg_tokenizer = RVQTokenizer()
rvq_eeg_tokenizer = rvq_eeg_tokenizer.to(config.system.DEVICE)
rvq_eeg_tokenizer.eval() # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •

v_original = llada_txt_tokenizer.vocab_size
eeg_seq_len = 12 # RVQ_N_Q
model_max_len = 512

unified_eeg_text_tokenizer = UnifiedEEGTextTokenizer(
    rvq_tokenizer_instance=rvq_eeg_tokenizer,
    llada_text_tokenizer_instance=llada_txt_tokenizer,
    max_seq_length=model_max_len,
    v_text_original=v_original,
    eeg_token_length=eeg_seq_len
)

eeg_llada_sft_model = EEGTextLLaDA(
    rvq_n_emb=rvq_eeg_tokenizer.n_emb, # RVQTokenizerì˜ n_embì™€ ì¼ì¹˜
    use_qlora=True
)

n_train = sum(p.numel() for p in eeg_llada_sft_model.parameters() if p.requires_grad)
print(f"Trainable params: {n_train/1e6:.2f} M")

# í‰ê°€ìš© ë°ì´í„°ë¡œë”ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì½œë ˆì´íŠ¸ í•¨ìˆ˜ ì˜ˆì‹œ
def collate_fn_for_evaluation(batch):
    eeg_data_list = [item[0] for item in batch] # ì›ë³¸ EEG ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    reference_texts_list = [item[1] for item in batch] # ì°¸ì¡° í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

    # EEG ë°ì´í„°ëŠ” ë°°ì¹˜ ë‚´ì—ì„œ íŒ¨ë”© ì—†ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ìœ ì§€í•˜ê±°ë‚˜,
    # ë§Œì•½ ëª¨ë“  EEG ë°ì´í„°ì˜ ê¸¸ì´ê°€ ê°™ë‹¤ë©´ torch.stackì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•˜ê³ , ìƒì„± ë£¨í”„ì—ì„œ ê°œë³„ ì²˜ë¦¬í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    return {
        "batched_eeg_data": eeg_data_list, 
        "batched_reference_texts": reference_texts_list
    }

# test_dataloader êµ¬ì„± ì‹œ ì´ ì½œë ˆì´íŠ¸ í•¨ìˆ˜ ì‚¬ìš©
# test_dataset = YourEEGDataset(...) # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤
# test_dataloader = DataLoader(
#     test_dataset,
#     batch_size=EVAL_BATCH_SIZE, # ì ì ˆí•œ ë°°ì¹˜ í¬ê¸° ì„¤ì •
#     collate_fn=collate_fn_for_evaluation
# )

# --- Pydantic configë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë¡œë”©, ë¶„í•  ë° ë°ì´í„°ë¡œë” ì„¤ì • ---
# ì´ì „ì— config = ExperimentConfig() ê°€ ì‹¤í–‰ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

# 1. EEGDataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (config ì‚¬ìš©)
# EEGDataset í´ë˜ìŠ¤ ì •ì˜ëŠ” ì´ë¯¸ ë…¸íŠ¸ë¶ì— ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
eeg_dataset = EEGDataset(data_dir=config.paths.DATASET_PATH)
eeg_dataset = eeg_dataset # í…ŒìŠ¤íŠ¸ìš©
num_total_samples = len(eeg_dataset)
indices = list(range(num_total_samples))

# 2. ë°ì´í„°ì…‹ ë¶„í•  (config ì‚¬ìš©)
# ì°¸ê³ : test_size ê°’ë“¤(í˜„ì¬ 0.2 ë° 0.5)ë„ config ê°ì²´ì— ì¶”ê°€í•˜ì—¬ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì˜ˆ: config.training.TRAIN_VAL_SPLIT_RATIO, config.training.VAL_TEST_SPLIT_RATIO
train_indices, temp_test_indices = train_test_split(
    indices, 
    test_size=0.2, # ì „ì²´ ë°ì´í„° ì¤‘ 20%ë¥¼ (ê²€ì¦+í…ŒìŠ¤íŠ¸)ìš©ìœ¼ë¡œ ë¶„ë¦¬
    random_state=config.system.SEED, # configì—ì„œ SEED ê°’ ì‚¬ìš©
    shuffle=True
)

val_indices, test_indices = train_test_split(
    temp_test_indices, 
    test_size=0.5, # (ê²€ì¦+í…ŒìŠ¤íŠ¸)ìš© ë°ì´í„° ì¤‘ 50%ë¥¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„ë¦¬ (ì¦‰, ì „ì²´ì˜ 10%)
    random_state=config.system.SEED, # configì—ì„œ SEED ê°’ ì‚¬ìš©
    shuffle=True
)

# ê° Subset ìƒì„±
train_dataset = Subset(eeg_dataset, train_indices)
val_dataset = Subset(eeg_dataset, val_indices)
test_dataset = Subset(eeg_dataset, test_indices)

print(f"ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°: {num_total_samples}")
print(f"í•™ìŠµ ì„¸íŠ¸ í¬ê¸°: {len(train_dataset)} (ì „ì²´ì˜ {len(train_dataset)/num_total_samples:.2%})")
print(f"ê²€ì¦ ì„¸íŠ¸ í¬ê¸°: {len(val_dataset)} (ì „ì²´ì˜ {len(val_dataset)/num_total_samples:.2%})")
print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í¬ê¸°: {len(test_dataset)} (ì „ì²´ì˜ {len(test_dataset)/num_total_samples:.2%})")

# DataCollatorForEEGTextSFT ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì´ì „ì— data_collator ë¡œ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •)
data_collator = DataCollatorForEEGTextSFT(unified_eeg_text_tokenizer)

# 3. DataLoader ìƒì„± (config ì‚¬ìš©)
# í•™ìŠµ ë°ì´í„° ë¡œë”
train_dataloader = DataLoader(
    train_dataset, 
    batch_size=config.training.BATCH_SIZE, # configì—ì„œ BATCH_SIZE ê°’ ì‚¬ìš©
    collate_fn=data_collator, 
    shuffle=True,
    num_workers=config.system.NUM_WORKERS, # configì—ì„œ NUM_WORKERS ê°’ ì‚¬ìš©
    pin_memory= False
)

# ê²€ì¦ ë°ì´í„° ë¡œë”
val_dataloader = DataLoader(
    val_dataset, 
    batch_size=config.training.BATCH_SIZE, # configì—ì„œ BATCH_SIZE ê°’ ì‚¬ìš©
    collate_fn=data_collator,
    shuffle=False,
    num_workers=config.system.NUM_WORKERS, # configì—ì„œ NUM_WORKERS ê°’ ì‚¬ìš©
    pin_memory= False
)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
test_dataloader = DataLoader(
    test_dataset, 
    batch_size=config.training.BATCH_SIZE, # configì—ì„œ BATCH_SIZE ê°’ ì‚¬ìš©
    collate_fn=collate_fn_for_evaluation,
    shuffle=False,
    num_workers=config.system.NUM_WORKERS, # configì—ì„œ NUM_WORKERS ê°’ ì‚¬ìš©
    pin_memory= False
)

print(f"\ní•™ìŠµ ë°ì´í„°ë¡œë” ë°°ì¹˜ ìˆ˜: {len(train_dataloader)}")
print(f"ê²€ì¦ ë°ì´í„°ë¡œë” ë°°ì¹˜ ìˆ˜: {len(val_dataloader)}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ë°°ì¹˜ ìˆ˜: {len(test_dataloader)}")

# 4. (ì„ íƒ ì‚¬í•­) ì˜µí‹°ë§ˆì´ì € ìƒì„± ì‹œì—ë„ config ì‚¬ìš©
optimizer = optim.AdamW(
    filter(lambda p: p.requires_grad, eeg_llada_sft_model.parameters()),
    lr=config.training.LEARNING_RATE
)

for g in optimizer.param_groups:
    for n, p in eeg_llada_sft_model.named_parameters():
        if any(p is param for param in g["params"]):   # `is` ë¡œ ê°ì²´ ë™ì¼ì„± ë¹„êµ
            if "wte" in n or n.endswith("ff_out.weight"):
                print("âœ… optimizer ì— í¬í•¨:", n)

print("\në°ì´í„° ë¡œë”© ë° ë¶„í•  (config ê¸°ë°˜) ì™„ë£Œ.")

print("--- ê°„ë‹¨ Main Loop ì‹œì‘ ---")

# config ê°ì²´ì—ì„œ ì—í­ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
# NUM_EPOCHS_TO_RUN = config.training.NUM_EPOCHS # ì „ì²´ ì—í­
NUM_EPOCHS_TO_RUN = 20
START_EPOCH_NUM = config.training.START_EPOCH # ì‹œì‘ ì—í­ (ë³´í†µ 0)
LLADA_MASK_TOKEN_ID = 126336
# ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì´ ê°„ë‹¨í•œ ë£¨í”„ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í•„ìš”ì‹œ ì¶”ê°€)
scheduler = None

CKPT_DIR = Path(config.paths.CKPT_DIR if hasattr(config, 'paths') and hasattr(config.paths, 'CKPT_DIR') else "./checkpoints_v4")
CKPT_DIR.mkdir(parents=True, exist_ok=True)

BEST_FINETUNED_MODEL_SAVE_DIR = CKPT_DIR / (config.paths.BEST_MODEL_COMPONENTS_DIR if hasattr(config, 'paths') and hasattr(config.paths, 'BEST_MODEL_COMPONENTS_DIR') else "best_finetuned_model_components")
BEST_MODEL_ADAPTER_SAVE_PATH = BEST_FINETUNED_MODEL_SAVE_DIR / "adapters"
BEST_MODEL_TRAINING_CHECKPOINT_SAVE_PATH = BEST_FINETUNED_MODEL_SAVE_DIR / "training_checkpoint.pt"


CSV_PATH   = CKPT_DIR / "loss_history.csv"
CSV_HEADER = ["epoch", "train_loss", "val_loss"]

peft = eeg_llada_sft_model.llada_model          # PeftModel
peft.modules_to_save = ["wte", "ff_out"]        # ì‹¤ì œ ëª¨ë“ˆëª…

# CSV íŒŒì¼ì´ ì²˜ìŒì´ë¼ë©´ í—¤ë” í•œ ë²ˆë§Œ ê¸°ë¡
if not CSV_PATH.exists():
    with CSV_PATH.open("w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(CSV_HEADER)

best_val_loss = float("inf")   # ë£¨í”„ ì „ì— ì´ˆê¸°í™”

for epoch_idx in range(START_EPOCH_NUM, START_EPOCH_NUM + NUM_EPOCHS_TO_RUN):
    current_epoch_display_num = epoch_idx + 1
    print(f"\n=== Epoch {current_epoch_display_num}/{START_EPOCH_NUM + NUM_EPOCHS_TO_RUN} ===")
    epoch_start_time = time.time()

    # 1. í•™ìŠµ ë£¨í”„ ì‹¤í–‰
    print("--- Training Phase --- ")
    train_loss = train_loop(
        model=eeg_llada_sft_model,
        train_dataloader=train_dataloader,
        optimizer=optimizer,
        device=config.system.DEVICE,
        current_epoch=current_epoch_display_num,
        total_epochs=START_EPOCH_NUM + NUM_EPOCHS_TO_RUN, # ì´ ë£¨í”„ì˜ ì´ ì—í­ ìˆ˜
        llada_mask_token_id=LLADA_MASK_TOKEN_ID,
        forward_process_fn=forward_process,
        gradient_accumulation_steps=config.training.GRADIENT_ACCUMULATION_STEPS,
        max_grad_norm=config.training.MAX_GRAD_NORM,
        scheduler=scheduler, # ì—¬ê¸°ì„œëŠ” None
        log_interval=config.training.TRAIN_LOG_INTERVAL
    )
    
    # ì…ë ¥ ì„ë² ë”© grad
    emb_grad = eeg_llada_sft_model.llada_model.get_input_embeddings().weight.grad
    print("emb grad â€–", None if emb_grad is None else emb_grad.norm())

    # LM-head(ff_out) grad
    lm_head = (
        eeg_llada_sft_model
            .llada_model               # PEFT ë˜í¼
            .base_model.model.model     # LLaDAModel
            .transformer.ff_out         # <-- ì—¬ê¸°!
    )
    lm_grad = lm_head.weight.grad
    print("ff_out grad â€–", None if lm_grad is None else lm_grad.norm())


    if math.isnan(train_loss):
        print(f"ERROR: Epoch {current_epoch_display_num} - í•™ìŠµ ì†ì‹¤ì´ NaNì…ë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        break

    # 2. ê²€ì¦ ë£¨í”„ ì‹¤í–‰
    print("--- Validation Phase --- ")
    val_loss = validation_loop(
        model=eeg_llada_sft_model,
        val_dataloader=val_dataloader,
        llada_mask_token_id=LLADA_MASK_TOKEN_ID,
        forward_process_fn=forward_process,
        device=config.system.DEVICE
    )

    if math.isnan(val_loss):
        print(f"ERROR: Epoch {current_epoch_display_num} - ê²€ì¦ ì†ì‹¤ì´ NaNì…ë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        break

    epoch_duration = time.time() - epoch_start_time
    print(f"Epoch {current_epoch_display_num} ì™„ë£Œ: í•™ìŠµ ì†ì‹¤ = {train_loss:.4f}, ê²€ì¦ ì†ì‹¤ = {val_loss:.4f}, ì†Œìš” ì‹œê°„ = {epoch_duration:.2f}s")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        print(f"New best validation loss: {best_val_loss:.4f}. Saving model components to {BEST_FINETUNED_MODEL_SAVE_DIR}...")
       # 1. PEFT ì–´ëŒ‘í„° ì €ì¥ (LoRA ê°€ì¤‘ì¹˜ ë° ë¦¬ì‚¬ì´ì¦ˆëœ ì„ë² ë”© í¬í•¨)
        BEST_MODEL_ADAPTER_SAVE_PATH.mkdir(parents=True, exist_ok=True)
        # 1. ëŒ€ìƒ ëª¨ë“ˆ ëª…ì‹œ
        model = eeg_llada_sft_model.llada_model

        # 3. ì €ì¥
        model.save_pretrained(str(BEST_MODEL_ADAPTER_SAVE_PATH), safe_serialization=True)
        print(f"  PEFT adapters saved to: {BEST_MODEL_ADAPTER_SAVE_PATH}")
        save_embedding_and_lm_head(model, BEST_FINETUNED_MODEL_SAVE_DIR)
        wte_path = BEST_FINETUNED_MODEL_SAVE_DIR / "wte.pth"
        lm_head_path = BEST_FINETUNED_MODEL_SAVE_DIR / "lm_head.pth"

        if wte_path.exists():
            wte_state = torch.load(wte_path, map_location="cpu")
            print(f"[wte.pth] âœ… ì €ì¥ í™•ì¸ - íŒŒë¼ë¯¸í„° ìˆ˜: {len(wte_state)}")
            print("  ì˜ˆì‹œ í‚¤:", list(wte_state.keys())[:3])
        else:
            print("[wte.pth] âŒ ì €ì¥ë˜ì§€ ì•ŠìŒ")

        if lm_head_path.exists():
            lm_head_state = torch.load(lm_head_path, map_location="cpu")
            print(f"[lm_head.pth] âœ… ì €ì¥ í™•ì¸ - íŒŒë¼ë¯¸í„° ìˆ˜: {len(lm_head_state)}")
            print("  ì˜ˆì‹œ í‚¤:", list(lm_head_state.keys())[:3])
        else:
            print("[lm_head.pth] âŒ ì €ì¥ë˜ì§€ ì•ŠìŒ")
        # === í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ===
        config_to_save = config.model_dump() if hasattr(config, 'model_dump') else None
        torch.save(
            {
                "epoch": current_epoch_display_num,
                "optimizer_state_dict": optimizer.state_dict(),
                "val_loss": val_loss,
                "config_dump": config_to_save
            },
            str(BEST_MODEL_TRAINING_CHECKPOINT_SAVE_PATH),
        )
        print(f"  Training checkpoint saved to: {BEST_MODEL_TRAINING_CHECKPOINT_SAVE_PATH}")
    
        # 2. í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì˜µí‹°ë§ˆì´ì € ìƒíƒœ, ì—í­, ì†ì‹¤, ì„¤ì • ë“±)
        # 'config'ëŠ” Pydantic ExperimentConfig ê°ì²´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
        config_to_save = None
        if 'config' in locals() and hasattr(config, 'model_dump'):
            config_to_save = config.model_dump()
        elif 'config' in globals() and hasattr(config, 'model_dump'): # ì „ì—­ ë³€ìˆ˜ì¼ ê²½ìš°
             config_to_save = config.model_dump()
        else:
            print("Warning: Pydantic 'config' object not found in local or global scope for saving.")
    
        torch.save(
            {
                "epoch": current_epoch_display_num,
                "optimizer_state_dict": optimizer.state_dict(),
                "val_loss": val_loss,
                "config_dump": config_to_save
            },
            str(BEST_MODEL_TRAINING_CHECKPOINT_SAVE_PATH),
        )
        print(f"  Training checkpoint saved to: {BEST_MODEL_TRAINING_CHECKPOINT_SAVE_PATH}")
        print(f"  [âœ“] Best model components updated at epoch {current_epoch_display_num}")

    # ----------------------------------------------------
    # Quick generation demo (after training is finished)
    # ----------------------------------------------------
    eeg_llada_sft_model.eval()                 # ì¶”ë¡  ëª¨ë“œ
    max_samples = 2                           # ë³´ê³  ì‹¶ì€ ìƒ˜í”Œ ê°œìˆ˜

    gen_texts, ref_texts = [], []

    with torch.no_grad():
        for i, batch in enumerate(test_dataloader, 1):
            eeg_list = batch["batched_eeg_data"]        # collate_fn_for_evaluation ì„ ì¼ë‹¤ë©´
            refs     = batch["batched_reference_texts"]

            for eeg_tensor, ref in zip(eeg_list, refs):
                gen = quick_generate(
                    model          = eeg_llada_sft_model.llada_model,  # PEFT ë˜í¼ ê·¸ëŒ€ë¡œ
                    tokenizer_wrap = unified_eeg_text_tokenizer,
                    eeg_tensor     = eeg_tensor.to(config.system.DEVICE),
                    gen_len   = 128,   # ìƒì„± í† í° ìˆ˜
                    steps     = 128,   # diffusion step
                    block     = 32,    # block length
                    temp      = 1.0,
                    cfg       = 2.0,
                )

                print("ğŸ”¹GT :", ref)
                print("ğŸ”¸GEN:", gen)
                print("-"*60)

                gen_texts.append(gen); ref_texts.append(ref)

            if i >= max_samples:      # ì¶©ë¶„íˆ ì¶œë ¥í–ˆìœ¼ë©´ ì¢…ë£Œ
                break

    
    # ---------- (2) ì†ì‹¤ CSV ì €ì¥ ----------
    with CSV_PATH.open("a", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([current_epoch_display_num, train_loss, val_loss])

    print(f"Epoch {current_epoch_display_num:03d} | train {train_loss:.4f} | val {val_loss:.4f}")

print("\n--- ê°„ë‹¨ Main Loop ì¢…ë£Œ ---")
