import torch
import torch.nn as nn
import torch.nn.functional as F # F.cross_entropyë¥¼ ìœ„í•´ ì¶”ê°€
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig # BitsAndBytesConfig ì¶”ê°€
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType # peft ê´€ë ¨ ëª¨ë“ˆ ì¶”ê°€
import pandas as pd
import shutil
import os
import numpy as np
from torch.nn import CrossEntropyLoss
from torch.utils.data import random_split
import random
import itertools
import gc
from tqdm import tqdm
import time

# ì‹œë“œ ê°’ ì„¤ì • (ì›í•˜ëŠ” ì •ìˆ˜ ê°’ìœ¼ë¡œ ì„¤ì •)
SEED = 42

def set_seeds(seed_value):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)  # ì—¬ëŸ¬ GPU ì‚¬ìš© ì‹œ

set_seeds(SEED)

class EEGDataset(Dataset):
    def __init__(self,
                 data_dir = "/home/work/skku/hyo/hyo/dataset/sentence.parquet"):
        df = pd.read_parquet(data_dir)
        eeg_vecs = df["eeg"].to_numpy()

        arr = np.stack(eeg_vecs).astype(np.float32)
        arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)
        mu, std = arr.mean(0, keepdims=True), arr.std(0, keepdims=True)+1e-8
        self.eeg_arr = (arr - mu) / std      # ì •ê·œí™”
        self.text_arr = df["text"].to_numpy() # í…ìŠ¤íŠ¸ ë°ì´í„°
        self.data = list(zip(torch.tensor(self.eeg_arr), self.text_arr))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

class ConvEEGEncoder(nn.Module):
    """
    840-dim ë²¡í„°ë¥¼ 1Ã—840 ì‹œí€€ìŠ¤ë¡œ ë³´ê³  Conv1D ë‘ ì¸µìœ¼ë¡œ ì ì¬í‘œí˜„ ìƒì„±
    ì¶œë ¥ì€ (B, latent_dim)
    """
    def __init__(self, input_dim=840, latent_dim=128, hidden=256):
        super().__init__()
        self.conv_stack = nn.Sequential(
            nn.Conv1d(1, hidden, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv1d(hidden, latent_dim, kernel_size=3, padding=1), nn.ReLU()
        )
        self.pool = nn.AdaptiveAvgPool1d(1)   # ê¸¸ì´ 840 â†’ 1 ë¡œ ì••ì¶•

    def forward(self, x):           # x: (B, feat)
        x = x.unsqueeze(1)          # (B, 1, 840)
        z = self.conv_stack(x)      # (B, latent_dim, 840)
        z = self.pool(z).squeeze(-1)  # (B, latent_dim)
        return z

class RVQ(nn.Module):
    def __init__(self, num_quantizers, num_embeddings, embedding_dim, commitment_cost=0.25):
        super().__init__()
        self.num_quantizers = num_quantizers # ì½”ë“œë¶ì˜ ê°œìˆ˜ (n_q)
        self.num_embeddings = num_embeddings # ê° ì½”ë“œë¶ ë‚´ ì„ë² ë”©(ì½”ë“œì›Œë“œ) ê°œìˆ˜ (n_emb, ì–´íœ˜ í¬ê¸°)
        self.embedding_dim = embedding_dim   # ê° ì„ë² ë”©ì˜ ì°¨ì› (D, latent_dimê³¼ ë™ì¼)
        self.commitment_cost = commitment_cost # VQ ì†ì‹¤ ê³„ì‚° ì‹œ ì‚¬ìš©ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°

        # num_quantizers ê°œìˆ˜ë§Œí¼ì˜ ì½”ë“œë¶(nn.Embedding ë ˆì´ì–´)ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì§
        self.codebooks = nn.ModuleList([
            nn.Embedding(self.num_embeddings, self.embedding_dim) for _ in range(self.num_quantizers)
        ])
        # ì½”ë“œë¶ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜í–‰)
        for i, codebook in enumerate(self.codebooks):
            nn.init.uniform_(codebook.weight, -1.0 / self.num_embeddings, 1.0 / self.num_embeddings)

    def forward(self, z_e): # ì…ë ¥ z_eì˜ ëª¨ì–‘: (B, L, D), ì—¬ê¸°ì„œ L=1, D=embedding_dim
        B, L, D = z_e.shape
        z_e_flat = z_e.reshape(-1, D) # (B*L, D) ëª¨ì–‘ìœ¼ë¡œ í¼ì¹¨ (ì—¬ê¸°ì„œëŠ” (B, D)ì™€ ë™ì¼)

        all_quantized_stages = [] # ê° ì½”ë“œë¶ì—ì„œ ì–‘ìí™”ëœ ë²¡í„°ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        all_indices = []          # ê° ì½”ë“œë¶ì—ì„œ ì„ íƒëœ ì¸ë±ìŠ¤ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        residual = z_e_flat       # ì²« ë²ˆì§¸ ì½”ë“œë¶ì— ì…ë ¥ë  ì”ì°¨ (ì´ˆê¸°ì—ëŠ” z_e_flat ì „ì²´)

        # num_quantizers ë§Œí¼ ë°˜ë³µ (ê° ì½”ë“œë¶ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬)
        for i in range(self.num_quantizers):
            codebook = self.codebooks[i] # í˜„ì¬ ì‚¬ìš©í•  ì½”ë“œë¶

            # í˜„ì¬ ì”ì°¨(residual)ì™€ í˜„ì¬ ì½”ë“œë¶ì˜ ëª¨ë“  ì„ë² ë”© ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì œê³± ê³„ì‚°
            # distances ëª¨ì–‘: (B*L, num_embeddings)
            distances = torch.sum(residual**2, dim=1, keepdim=True) \
                        - 2 * torch.matmul(residual, codebook.weight.t()) \
                        + torch.sum(codebook.weight**2, dim=1, keepdim=True).t()

            # ê°€ì¥ ê°€ê¹Œìš´ ì„ë² ë”©ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            # current_indices ëª¨ì–‘: (B*L)
            current_indices = torch.argmin(distances, dim=1)
            all_indices.append(current_indices) # í˜„ì¬ ì½”ë“œë¶ì˜ ì¸ë±ìŠ¤ ì €ì¥

            # ì„ íƒëœ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–‘ìí™”ëœ ë²¡í„°(ì½”ë“œì›Œë“œ) ê°€ì ¸ì˜¤ê¸°
            # quantized_vector ëª¨ì–‘: (B*L, D)
            quantized_vector = codebook(current_indices)
            # ì›ë˜ ëª¨ì–‘ (B, L, D)ë¡œ ë³µì›í•˜ì—¬ ì €ì¥ (ì—¬ê¸°ì„œëŠ” (B, 1, D))
            all_quantized_stages.append(quantized_vector.reshape(B, L, D))

            # ë‹¤ìŒ ì½”ë“œë¶ìœ¼ë¡œ ë„˜ê¸¸ ì”ì°¨ ê³„ì‚°
            # ì¤‘ìš”: quantized_vectorì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ì„ ëŠê¸° ìœ„í•´ .detach() ì‚¬ìš©
            residual = residual - quantized_vector.detach()

        # ëª¨ë“  ì½”ë“œë¶ì—ì„œ ë‚˜ì˜¨ ì–‘ìí™”ëœ ë²¡í„°ë“¤ì„ í•©ì‚° (EEGTran ë…¼ë¬¸ Figure 2 ì°¸ì¡°)
        # final_quantized_output ëª¨ì–‘: (B, L, D)
        final_quantized_output = torch.stack(all_quantized_stages, dim=0).sum(dim=0)

        # ìˆ˜ì§‘ëœ ì¸ë±ìŠ¤ë“¤ì„ (B, L, num_quantizers) í˜•íƒœë¡œ ìŒ“ìŒ
        # stacked_indices ëª¨ì–‘: (B, L, n_q) (ì—¬ê¸°ì„œëŠ” (B, 1, n_q))
        stacked_indices = torch.stack(all_indices, dim=1).reshape(B, L, self.num_quantizers)

        # ìµœì¢… ë°˜í™˜ê°’: í•©ì‚°ëœ ì–‘ìí™” ë²¡í„°, ìŒ“ì¸ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤, VQ ì†ì‹¤
        # RVQTokenizerì˜ forwardì—ì„œëŠ” ì´ ì¤‘ ì²« ë‘ ê°œë¥¼ zq, indicesë¡œ ë°›ê²Œ ë©ë‹ˆë‹¤.
        return final_quantized_output, stacked_indices

class RVQTokenizer(nn.Module):
    def __init__(self,
                 feat=840,
                 latent=128,  # 1024->2048
                 n_q=12,
                 n_emb=512,
                 hidden=256,
                 TOKENIZER_CHECKPOINT_PATH = "/home/work/skku/hyo/hyo/model/rvq_best_model_sen_512.pt"
                 ):
        super().__init__()
        self.n_q = n_q
        self.n_emb = n_emb
        # ì‹¤ì œ ConvEEGEncoderì™€ RVQ ëª¨ë“ˆì´ ì—¬ê¸°ì— ì™€ì•¼ í•¨
        self.enc = ConvEEGEncoder(feat, latent, hidden)
        self.rvq = RVQ(num_quantizers=n_q, num_embeddings=n_emb, embedding_dim=latent)

        checkpoint = torch.load(TOKENIZER_CHECKPOINT_PATH, map_location="cpu")
        self.enc.load_state_dict(checkpoint["encoder"])
        for i, cb_weight_tensor in enumerate(checkpoint["codebooks"]):
            self.rvq.codebooks[i].weight.data = cb_weight_tensor

    @torch.no_grad()
    def forward(self, x): # x: (B, 840)
        z = self.enc(x)
        quantized_vector, token_indices = self.rvq(z.unsqueeze(1)) # vq_lossëŠ” ë¬´ì‹œ
        zq = quantized_vector
        indices = token_indices # ëª¨ì–‘ (B, 1, n_q)
        # ë§Œì•½ LLaDA ì…ë ¥ìš©ìœ¼ë¡œ (B, n_q) ëª¨ì–‘ì˜ ì¸ë±ìŠ¤ë¥¼ ì›í•œë‹¤ë©´ squeeze(1) í•„ìš”
        # return zq, indices.squeeze(1)
        return zq, indices # í˜„ì¬ pasted_content.txtì˜ ì£¼ì„ê³¼ ë§ì¶”ë ¤ë©´ ì´ëŒ€ë¡œ

def forward_process_eeg(original_eeg_ids, mask_eeg_token_id = 126336, eps=1e-3):
    # original_eeg_ids: (B, n_q) ëª¨ì–‘ì˜ EEG í† í° ID
    # mask_eeg_token_id: ìš°ë¦¬ê°€ ì •ì˜í•œ EEG ë§ˆìŠ¤í¬ í† í° ID (ì˜ˆ: RVQ_N_EMB)
    b, l = original_eeg_ids.shape

    # ê° ë°°ì¹˜ ìƒ˜í”Œë³„ë¡œ ëœë¤í•œ t ê°’ì„ ìƒì„± (0~1)
    # LLaDA ì½”ë“œëŠ” të¥¼ (b)ë¡œ ë§Œë“¤ì§€ë§Œ, ë…¼ë¬¸ Figure 2aëŠ” t ~ U(0,1)ë¡œ ë‹¨ì¼ ê°’ì„ ì˜ë¯¸í•˜ê¸°ë„ í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” LLaDA ì½”ë“œ ìŠ¤íƒ€ì¼ì„ ë”°ë¼ ë°°ì¹˜ë³„ të¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    t_per_sample = torch.rand(b, device=original_eeg_ids.device)

    # p_mask ê³„ì‚°: ê° ìƒ˜í”Œì˜ t ê°’ì— ë”°ë¼ í•´ë‹¹ ìƒ˜í”Œ ë‚´ ëª¨ë“  í† í°ì— ì ìš©ë  ë§ˆìŠ¤í‚¹ í™•ë¥ 
    # p_mask_per_sampleì˜ ëª¨ì–‘: (b, 1)
    p_mask_per_sample = (1 - eps) * t_per_sample + eps
    # p_mask_for_tokensì˜ ëª¨ì–‘: (b, l)
    p_mask_for_tokens = p_mask_per_sample.unsqueeze(-1).repeat(1, l)

    # ê° í† í° ìœ„ì¹˜ë³„ë¡œ ë§ˆìŠ¤í‚¹ ì—¬ë¶€ ê²°ì •
    # noise_for_maskingì˜ ëª¨ì–‘: (b, l)
    noise_for_masking = torch.rand((b, l), device=original_eeg_ids.device)
    masked_indices = noise_for_masking < p_mask_for_tokens # Trueë©´ ë§ˆìŠ¤í¬

    # ë§ˆìŠ¤í¬ëœ ì…ë ¥ ìƒì„± (noisy_batch ì—­í• )
    masked_eeg_ids_for_input = torch.where(masked_indices, mask_eeg_token_id, original_eeg_ids)

    return masked_eeg_ids_for_input, masked_indices # p_maskëŠ” ì§ì ‘ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ë°˜í™˜ ì•ˆ í•¨ (í•„ìš”ì‹œ ì¶”ê°€)

class EEG_LLaDA_MLM(nn.Module):
    def __init__(self, llada_model_name, rvq_n_emb, use_qlora=True, qlora_config_params=None):
        super().__init__()
        self.rvq_n_emb = rvq_n_emb
        self.llada_model_name = llada_model_name

        bnb_config = None
        if use_qlora:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16, # ë˜ëŠ” torch.float16
                bnb_4bit_use_double_quant=True,
            )

        # LLaDA ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” ì„¤ì • ì ìš©)
        self.llada_model = AutoModelForCausalLM.from_pretrained(
            llada_model_name,
            quantization_config=bnb_config if use_qlora else None,
            torch_dtype=torch.bfloat16 if use_qlora and bnb_config else "auto", # ì–‘ìí™” ì‹œ bfloat16 ì‚¬ìš© ê¶Œì¥
            trust_remote_code=True,
            # device_map="auto" # ì—¬ëŸ¬ GPU ì‚¬ìš© ì‹œ ë˜ëŠ” ë©”ëª¨ë¦¬ ìµœì í™” ì‹œ ê³ ë ¤
        )
        self.llada_hidden_size = self.llada_model.config.hidden_size
        model_dtype = self.llada_model.dtype

        self.v_text = self.llada_model.config.vocab_size
        num_new_eeg_tokens = self.rvq_n_emb + 1
        new_total_vocab_size = self.v_text + num_new_eeg_tokens
        print(f"Original vocab size: {self.v_text}")
        print(f"Resizing token embeddings to: {new_total_vocab_size}")
        self.llada_model.resize_token_embeddings(new_total_vocab_size)        
        self.global_mask_eeg_token_id = self.v_text + self.rvq_n_emb

# # ----------------------------------------------  FIX  ---------------------------------------------- #
#         # â€†text ìª½ì€ 0 â€¦ self.v_text-1,                  
#         # â€†EEG í† í°ì€ self.eeg_token_offset â€¦ new_total_vocab_size-2,  
#         # â€†ë§ˆìŠ¤í¬ í† í°ì€ **ë§¨ ë§ˆì§€ë§‰ í–‰ ë²ˆí˜¸**.
#         self.eeg_token_offset = self.v_text              # = ì›ë˜ vocab_size
#         self.global_mask_eeg_token_id = new_total_vocab_size - 1 # = ë§ˆì§€ë§‰ index (= í–‰ ìˆ˜-1)
# # ---------------------------------------------------------------------------------------------------- #        

        # QLoRA ì ìš©
        if use_qlora:
            # ëª¨ë¸ì„ k-bit í•™ìŠµìš©ìœ¼ë¡œ ì¤€ë¹„ (ì–‘ìí™”ëœ ëª¨ë¸ì— í•„ìš”)
            #self.llada_model = prepare_model_for_kbit_training(self.llada_model)

            # LoRA ì„¤ì • ì •ì˜
            # target_modulesëŠ” ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸ í•„ìš” (ì•„ë˜ ì„¤ëª… ì°¸ì¡°)
            default_target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
            if qlora_config_params and "target_modules" in qlora_config_params:
                target_modules = qlora_config_params["target_modules"]
            else:
                target_modules = default_target_modules

            lora_config = LoraConfig(
                r=qlora_config_params.get("r", 16) if qlora_config_params else 16, # LoRA rank
                lora_alpha=qlora_config_params.get("lora_alpha", 32) if qlora_config_params else 32, # Alpha scaling
                target_modules=target_modules,
                lora_dropout=qlora_config_params.get("lora_dropout", 0.05) if qlora_config_params else 0.05,
                bias="none", # LoRAëŠ” ë³´í†µ biasë¥¼ í•™ìŠµí•˜ì§€ ì•ŠìŒ
                task_type=TaskType.CAUSAL_LM, # Causal LM ì‘ì—…ìš©
            )
            self.llada_model = get_peft_model(self.llada_model, lora_config)
            print("QLoRA applied to LLaDA model.")

            print("Making input embeddings trainable for newly added tokens...")
            if hasattr(self.llada_model, 'base_model'): # PeftModel ê²½ìš°
                embedding_layer = self.llada_model.base_model.get_input_embeddings()
            else: # ì¼ë°˜ ëª¨ë¸ ê²½ìš° (get_peft_model ì´ì „)
                embedding_layer = self.llada_model.get_input_embeddings()

            for param in embedding_layer.parameters():
                param.requires_grad = True
            print("Input embeddings are now trainable.")

            self.llada_model.print_trainable_parameters() # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥


        self.mlm_head = nn.Linear(self.llada_hidden_size, self.rvq_n_emb, dtype=model_dtype)

    def forward(self, masked_global_eeg_ids_for_input, attention_mask=None, mlm_labels=None):
        model_outputs = self.llada_model(
            input_ids=masked_global_eeg_ids_for_input,
            attention_mask=attention_mask,
            output_hidden_states=True,  # ì¤‘ê°„ ì€ë‹‰ ìƒíƒœë“¤ì„ ì¶œë ¥í•˜ë„ë¡ ìš”ì²­
            return_dict=True
        )

        # output_hidden_states=Trueë¡œ ì„¤ì •í•˜ë©´, model_outputs.hidden_states ì— ëª¨ë“  ë ˆì´ì–´ì˜ ì€ë‹‰ ìƒíƒœê°€ íŠœí”Œ í˜•íƒœë¡œ ì €ì¥ë©ë‹ˆë‹¤.
        # ì´ íŠœí”Œì˜ ë§ˆì§€ë§‰ ìš”ì†Œê°€ ìš°ë¦¬ê°€ ì›í•˜ëŠ” last_hidden_state ì…ë‹ˆë‹¤.
        # (ì…ë ¥ ì„ë² ë”© ê²°ê³¼ + ê° íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ì˜ ì¶œë ¥ ê²°ê³¼)
        all_hidden_states = model_outputs.hidden_states
        sequence_output = all_hidden_states[-1] # ë§ˆì§€ë§‰ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ì˜ ì¶œë ¥

        # --- ë””ë²„ê¹…ì„ ìœ„í•œ print ë¬¸ (ì—¬ì „íˆ ìœ íš¨í•©ë‹ˆë‹¤) --- #
        #print(f"Shape of sequence_output (from hidden_states[-1]) before mlm_head: {sequence_output.shape}")
        # ì´ì œ ì˜ˆìƒë˜ëŠ” ëª¨ì–‘: (batch_size, sequence_length, llada_hidden_size), ì˜ˆ: (1, 64, 4096)

        mlm_logits = self.mlm_head(sequence_output)

        loss = None
        if mlm_labels is not None:
            loss_fct = CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(mlm_logits.view(-1, self.rvq_n_emb), mlm_labels.view(-1))

        return {
            "loss": loss,
            "logits": mlm_logits,
            # "hidden_states": sequence_output # í•„ìš”í•˜ë‹¤ë©´ ì „ì²´ hidden_states íŠœí”Œì„ ë°˜í™˜í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
        }

def evaluate_model(model, dataloader, device, rvq_tokenizer):
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    total_val_loss = 0
    
    with torch.no_grad(): # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        for batch_eeg_tensors in dataloader:
            batch_eeg_tensors = batch_eeg_tensors.to(device)

            # 1. RVQ í† í°í™”
            _, local_eeg_indices_batch = rvq_tokenizer(batch_eeg_tensors)
            original_local_eeg_ids = local_eeg_indices_batch.squeeze(1)
            
            # 2. ê¸€ë¡œë²Œ ID ë³€í™˜ ë° ë§ˆìŠ¤í‚¹
            global_original_eeg_ids = original_local_eeg_ids + model.v_text
#             global_original_eeg_ids = original_local_eeg_ids + model.eeg_token_offset
            masked_global_eeg_ids_for_input, masked_indices = forward_process_eeg(
                global_original_eeg_ids, 
                model.global_mask_eeg_token_id
            )

            # 3. MLM ë ˆì´ë¸” ìƒì„±
            mlm_labels = original_local_eeg_ids.clone()
            mlm_labels[~masked_indices] = -100

            # 4. ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
            attention_mask = torch.ones_like(original_local_eeg_ids, dtype=torch.float32, device=device)

#             # ------------------- â‘  í† í° id ë²”ìœ„ ê²€ì‚¬ ------------------- #
#             vocab_sz = model.llada_model.get_input_embeddings().weight.size(0)
#             if masked_global_eeg_ids_for_input.max() >= vocab_sz:
#                 bad_ids = masked_global_eeg_ids_for_input[
#                     masked_global_eeg_ids_for_input >= vocab_sz]
#                 raise RuntimeError(
#                     f"[BUG] found out-of-range ids {bad_ids.tolist()}  (vocab={vocab_sz})")
#             # ---------------------------------------------------------- #

            # 5. ëª¨ë¸ ìˆœì „íŒŒ ë° ì†ì‹¤ ê³„ì‚°
            outputs = model(
                masked_global_eeg_ids_for_input=masked_global_eeg_ids_for_input,
                attention_mask=attention_mask,
                mlm_labels=mlm_labels
            )
            loss = outputs["loss"]
            if loss is not None:
                total_val_loss += loss.item()
            else:
                print("ê²€ì¦ ì¤‘ ì†ì‹¤ì´ Noneì…ë‹ˆë‹¤.") # ì´ ê²½ìš°ëŠ” ê±°ì˜ ì—†ì–´ì•¼ í•¨

    avg_val_loss = total_val_loss / len(dataloader)
    model.train() # ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
    return avg_val_loss

def collate_fn_eeg_mlm(batch):
    eeg_tensors = [item[0] for item in batch] # item[0]ì´ eeg_tensorë¼ê³  ê°€ì •
    return torch.stack(eeg_tensors)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
LLADA_MODEL_NAME = "GSAI-ML/LLaDA-8B-Base" # ë˜ëŠ” ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ëª…
RVQ_N_EMB = 512  # RVQ ì½”ë“œë¶ì˜ ì„ë² ë”© ê°œìˆ˜ (ì–´íœ˜ í¬ê¸°)
RVQ_N_Q = 12     # RVQ ì½”ë“œë¶ ê°œìˆ˜ (í† í° ì‹œí€€ìŠ¤ ê¸¸ì´)
BATCH_SIZE = 64   # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
INIT_LR = 1e-4   # ì´ˆê¸° í•™ìŠµë¥ 
NUM_EPOCHS = 10   # í•™ìŠµ ì—í­ ìˆ˜
VALIDATION_SPLIT = 0.1
max_grad_norm = 1.0
model_save_path_base = "/home/work/skku/hyo/hyo/model/eeg_llada_mlm_model"
grid_search_results = []

GRID_SEARCH_BASE_DIR = "/home/work/skku/hyo/hyo/grid_search_results" # ëª¨ë“  ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ì €ì¥ ê¸°ë³¸ í´ë”
os.makedirs(GRID_SEARCH_BASE_DIR, exist_ok=True)
OVERALL_BEST_MODEL_DIR = os.path.join(GRID_SEARCH_BASE_DIR, "overall_best_model")
if os.path.exists(OVERALL_BEST_MODEL_DIR):
    shutil.rmtree(OVERALL_BEST_MODEL_DIR)
os.makedirs(OVERALL_BEST_MODEL_DIR, exist_ok=True)

print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE}")

rvq_tokenizer = RVQTokenizer(
    feat=840, 
    latent=128, # RVQ ë‚´ë¶€ ì„ë² ë”© ì°¨ì›, LLaDA hidden sizeì™€ ë‹¤ë¦„
    n_q=RVQ_N_Q, 
    n_emb=RVQ_N_EMB, 
    hidden=256,
    TOKENIZER_CHECKPOINT_PATH="/home/work/skku/hyo/hyo/model/rvq_best_model_sen_512.pt" # ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •!
).to(DEVICE)
rvq_tokenizer.eval() # í† í¬ë‚˜ì´ì €ëŠ” í•™ìŠµí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ eval ëª¨ë“œ

eeg_dataset_full = EEGDataset(data_dir="/home/work/skku/hyo/hyo/dataset/sentence.parquet") # ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •!

param_grid = {
    'learning_rate': [1e-4],
    'lora_r': [8, 16, 32],
    'lora_alpha': [16, 32, 64],
    'batch_size': [64], # GPU ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¼ ì¡°ì ˆ
    'validation_split' : [0.1]
}

keys, values = zip(*param_grid.items())
hyperparameter_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]

print(f"ì´ {len(hyperparameter_combinations)}ê°œì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ê·¸ë¦¬ë“œ ì„œì¹˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")

all_epoch_logs_list = [] # ëª¨ë“  ì—í­ ë¡œê·¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
overall_best_val_loss = float('inf')

# ê·¸ë¦¬ë“œ ì„œì¹˜ ì¡°í•©ë“¤ì— ëŒ€í•œ tqdm í”„ë¡œê·¸ë ˆìŠ¤ ë°”
combo_pbar = tqdm(enumerate(hyperparameter_combinations), 
                  total=len(hyperparameter_combinations),
                  desc="Grid Search Progress",
                  unit="combo")

for combo_idx, params in combo_pbar:
    combo_id_str = f"combo_{combo_idx+1:03d}_lr_{params['learning_rate']}_r_{params['lora_r']}_alpha_{params['lora_alpha']}_bs_{params['batch_size']}"
    
    # tqdm ì„¤ëª… ì—…ë°ì´íŠ¸
    combo_pbar.set_description(f"Combo {combo_idx+1}/{len(hyperparameter_combinations)}")
    combo_pbar.set_postfix({
        'lr': f"{params['learning_rate']:.2e}",
        'r': params['lora_r'],
        'alpha': params['lora_alpha'],
        'bs': params['batch_size']
    })
    
    print(f"\n--- ê·¸ë¦¬ë“œ ì„œì¹˜ ì¡°í•© {combo_idx+1}/{len(hyperparameter_combinations)} ({combo_id_str}) ì‹œì‘ ---")
    print(f"í˜„ì¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {params}")

    current_combo_save_dir = os.path.join(GRID_SEARCH_BASE_DIR, combo_id_str)
    os.makedirs(current_combo_save_dir, exist_ok=True)

    current_lr = params['learning_rate']
    current_lora_r = params['lora_r']
    current_lora_alpha = params['lora_alpha']
    current_batch_size = params['batch_size']
    current_validation_slplit = params['validation_split']

    set_seeds(SEED) 

    # ë°ì´í„°ì…‹ ë¶„í• 
    dataset_size = len(eeg_dataset_full)
    val_size = int(dataset_size * current_validation_slplit)
    train_size = dataset_size - val_size
    
    print(f"ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°: {dataset_size}")
    print(f"í•™ìŠµ ë°ì´í„°ì…‹ í¬ê¸°: {train_size}")
    print(f"ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: {val_size}")
    
    # random_splitì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ë¶„í•  (ì‹œë“œ ê³ ì •ìœ¼ë¡œ ì¬í˜„ì„± í™•ë³´ ê°€ëŠ¥)
    # torch.manual_seed(42) # í•„ìš”ì‹œ ì‹œë“œ ê³ ì •
    train_dataset, val_dataset = random_split(eeg_dataset_full, [train_size, val_size])

    train_dataloader = DataLoader(train_dataset, batch_size=current_batch_size, shuffle=True, collate_fn=collate_fn_eeg_mlm, worker_init_fn=None)
    val_dataloader = DataLoader(val_dataset, batch_size=current_batch_size, shuffle=False, collate_fn=collate_fn_eeg_mlm, worker_init_fn=None)
    
    print(f"í•™ìŠµ ë°ì´í„°ë¡œë” í¬ê¸°: {len(train_dataloader)}")
    print(f"ê²€ì¦ ë°ì´í„°ë¡œë” í¬ê¸°: {len(val_dataloader)}")
    
    # EEG_LLaDA_MLM ëª¨ë¸ ì´ˆê¸°í™” (ì´ì „ ì½”ë“œì—ì„œ ì •ì˜ëœ í´ë˜ìŠ¤ ì‚¬ìš©)
    qlora_params_config = {
        "r": current_lora_r,
        "lora_alpha": current_lora_alpha,
        "lora_dropout": 0.05,
        "target_modules": ["q_proj", "v_proj"] # LLaDA ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ ëª…ì‹œì  ì§€ì • ê¶Œì¥ (ì´ì „ ì•ˆë‚´ ì°¸ì¡°)
    }
    model = EEG_LLaDA_MLM(
        llada_model_name=LLADA_MODEL_NAME, 
        rvq_n_emb=RVQ_N_EMB, 
        use_qlora=True,
        qlora_config_params=qlora_params_config
    ).to(DEVICE)

    params_to_optimize = []
    print("\nì˜µí‹°ë§ˆì´ì €ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ìˆ˜ì§‘ ì¤‘:")
    for name, param in model.llada_model.named_parameters():
        if param.requires_grad:
            params_to_optimize.append(param)
            #print(f"  LLaDA (PEFT): {name} (ëª¨ì–‘: {param.shape}, dtype: {param.dtype})")
    
    for name, param in model.mlm_head.named_parameters():
        if param.requires_grad:
            params_to_optimize.append(param)
            #print(f"  MLM í—¤ë“œ: {name} (ëª¨ì–‘: {param.shape}, dtype: {param.dtype})")
    
    if not params_to_optimize:
        raise ValueError("í•™ìŠµí•  íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    print(f"ì˜µí‹°ë§ˆì´ì €ë¥¼ ìœ„í•œ ì´ íŒŒë¼ë¯¸í„° ê·¸ë£¹ ìˆ˜: {len(params_to_optimize)}")
    
    optimizer = optim.AdamW(params_to_optimize, lr=current_lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)
    
    print("\nì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ.")

    best_val_loss_this_combo = float('inf')

    # ì—í­ ì§„í–‰ì„ ìœ„í•œ tqdm í”„ë¡œê·¸ë ˆìŠ¤ ë°”
    epoch_pbar = tqdm(range(NUM_EPOCHS), 
                      desc=f"Epochs (Combo {combo_idx+1})",
                      leave=False,
                      unit="epoch")

    for epoch in epoch_pbar:
        model.train()
        total_train_loss_epoch = 0
        epoch_start_time = time.time()

        # í•™ìŠµ ë°°ì¹˜ë“¤ì— ëŒ€í•œ tqdm í”„ë¡œê·¸ë ˆìŠ¤ ë°”
        train_pbar = tqdm(train_dataloader, 
                         desc=f"Training Epoch {epoch+1}",
                         leave=False,
                         unit="batch")
                         
        for step, batch_eeg_tensors in enumerate(train_pbar):
            batch_eeg_tensors = batch_eeg_tensors.to(DEVICE)
            with torch.no_grad():
                _, local_eeg_indices_batch = rvq_tokenizer(batch_eeg_tensors)
                original_local_eeg_ids = local_eeg_indices_batch.squeeze(1)
            global_original_eeg_ids = original_local_eeg_ids + model.v_text
            masked_global_eeg_ids_for_input, masked_indices = forward_process_eeg(
                global_original_eeg_ids, model.global_mask_eeg_token_id)
            mlm_labels = original_local_eeg_ids.clone()
            mlm_labels[~masked_indices] = -100
            attention_mask = torch.ones_like(original_local_eeg_ids, dtype=torch.float32, device=DEVICE)
            optimizer.zero_grad()
            outputs = model(masked_global_eeg_ids_for_input=masked_global_eeg_ids_for_input,
                            attention_mask=attention_mask, mlm_labels=mlm_labels)
            loss = outputs["loss"]
            if loss is None: continue
            loss.backward()
            torch.nn.utils.clip_grad_norm_(params_to_optimize, max_grad_norm)
            optimizer.step()
            total_train_loss_epoch += loss.item()

            # í˜„ì¬ê¹Œì§€ì˜ í‰ê·  ì†ì‹¤ì„ í‘œì‹œ
            current_avg_loss = total_train_loss_epoch / (step + 1)
            train_pbar.set_postfix({'loss': f'{current_avg_loss:.4f}'})
        
        train_pbar.close()
        
        avg_train_loss_epoch = total_train_loss_epoch / len(train_dataloader)

        print(f"  ê²€ì¦ ì¤‘...")
        avg_val_loss_epoch = evaluate_model(model, val_dataloader, DEVICE, rvq_tokenizer) # evaluate_model í•¨ìˆ˜ëŠ” ì´ì „ì— ì •ì˜ë¨

        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        
        # ì—í­ í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
        epoch_pbar.set_postfix({
            'train_loss': f'{avg_train_loss_epoch:.4f}',
            'val_loss': f'{avg_val_loss_epoch:.4f}',
            'lr': f'{optimizer.param_groups[0]["lr"]:.2e}',
            'time': f'{epoch_duration:.1f}s'
        })
        
        print(f"  ì¡°í•© {combo_idx+1}, ì—í­ {epoch+1}: Train Loss={avg_train_loss_epoch:.4f}, Val Loss={avg_val_loss_epoch:.4f}, LR={optimizer.param_groups[0]['lr']:.2e}")
        scheduler.step(avg_val_loss_epoch)

        # CSV ë¡œê¹…ì„ ìœ„í•œ ë°ì´í„° ì¶”ê°€
        epoch_log_entry = params.copy() # í˜„ì¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³µì‚¬
        epoch_log_entry['combo_id_str'] = combo_id_str
        epoch_log_entry['combo_idx'] = combo_idx + 1
        epoch_log_entry['epoch'] = epoch + 1
        epoch_log_entry['train_loss'] = avg_train_loss_epoch
        epoch_log_entry['validation_loss'] = avg_val_loss_epoch
        epoch_log_entry['current_lr_epoch_end'] = optimizer.param_groups[0]['lr']
        all_epoch_logs_list.append(epoch_log_entry)

        # ë§¤ ì—í­ ëª¨ë¸ ì €ì¥
        epoch_model_save_dir = os.path.join(current_combo_save_dir, f"epoch_{epoch+1}")
        os.makedirs(epoch_model_save_dir, exist_ok=True)
        model.llada_model.save_pretrained(os.path.join(epoch_model_save_dir, "qlora_adapter"))
        torch.save(model.mlm_head.state_dict(), os.path.join(epoch_model_save_dir, "mlm_head.pth"))
        print(f"    ì—í­ {epoch+1} ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {epoch_model_save_dir}")

        # í˜„ì¬ ì¡°í•© ë‚´ì—ì„œ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì—…ë°ì´íŠ¸ ë° ì €ì¥
        if avg_val_loss_epoch < best_val_loss_this_combo:
            best_val_loss_this_combo = avg_val_loss_epoch
            combo_best_model_save_dir = os.path.join(current_combo_save_dir, "best_model_in_combo")
            os.makedirs(combo_best_model_save_dir, exist_ok=True)
            model.llada_model.save_pretrained(os.path.join(combo_best_model_save_dir, "qlora_adapter"))
            torch.save(model.mlm_head.state_dict(), os.path.join(combo_best_model_save_dir, "mlm_head.pth"))
            print(f"    ì¡°í•© ë‚´ ë² ìŠ¤íŠ¸ ëª¨ë¸ ê°±ì‹  (ì—í­ {epoch+1}), Val Loss: {best_val_loss_this_combo:.4f}. ì €ì¥ ì™„ë£Œ: {combo_best_model_save_dir}")

        # ì „ì²´ ê·¸ë¦¬ë“œ ì„œì¹˜ ì¤‘ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì—…ë°ì´íŠ¸ ë° ì €ì¥
        if avg_val_loss_epoch < overall_best_val_loss:
            overall_best_val_loss = avg_val_loss_epoch
            print(f"    âœ¨ ì „ì²´ ë² ìŠ¤íŠ¸ ëª¨ë¸ ê°±ì‹  (ì¡°í•© {combo_idx+1}, ì—í­ {epoch+1}), Val Loss: {overall_best_val_loss:.4f}. ì €ì¥ ì¤‘...")
            # if os.path.exists(OVERALL_BEST_MODEL_DIR): # ì´ì „ ë² ìŠ¤íŠ¸ ëª¨ë¸ í´ë” ì‚­ì œ
            #     shutil.rmtree(OVERALL_BEST_MODEL_DIR)
            # os.makedirs(OVERALL_BEST_MODEL_DIR, exist_ok=True) # ì‚­ì œ í›„ ë‹¤ì‹œ ìƒì„±
            model.llada_model.save_pretrained(os.path.join(OVERALL_BEST_MODEL_DIR, "qlora_adapter"))
            torch.save(model.mlm_head.state_dict(), os.path.join(OVERALL_BEST_MODEL_DIR, "mlm_head.pth"))
            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì •ë³´ ì €ì¥ (ì–´ë–¤ ì¡°í•©ê³¼ ì—í­ì´ì—ˆëŠ”ì§€)
            with open(os.path.join(OVERALL_BEST_MODEL_DIR, "best_model_info.txt"), "w") as f:
                f.write(f"Best model from combination: {combo_id_str}\n")
                f.write(f"Epoch: {epoch+1}\n")
                f.write(f"Validation Loss: {overall_best_val_loss:.4f}\n")
                f.write(f"Hyperparameters: {params}\n")
            print(f"    âœ¨ ì „ì²´ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {OVERALL_BEST_MODEL_DIR}")
            
    epoch_pbar.close()
    print(f"--- ê·¸ë¦¬ë“œ ì„œì¹˜ ì¡°í•© {combo_idx+1} ì™„ë£Œ. ì´ ì¡°í•©ì˜ ìµœì € ê²€ì¦ ì†ì‹¤: {best_val_loss_this_combo:.4f} ---")

    # --- ë©”ëª¨ë¦¬ í•´ì œ ì‹œì‘ ---
    print(f"ì¡°í•© {combo_idx+1}ì— ì‚¬ìš©ëœ ê°ì²´ë“¤ì˜ ë©”ëª¨ë¦¬ í•´ì œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
    # 1. ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì‚­ì œ
    del model
    del optimizer
    del scheduler
    # í•„ìš”í•˜ë‹¤ë©´ ë°ì´í„°ë¡œë”ë„ ì‚­ì œ (ë§Œì•½ ë£¨í”„ ë‚´ì—ì„œ ë§¤ë²ˆ ì¬ìƒì„±ëœë‹¤ë©´)
    # del train_dataloader
    # del val_dataloader 
    # (ì£¼ì˜: train_dataset, val_datasetì€ random_splitìœ¼ë¡œ ìƒì„±ë˜ë¯€ë¡œ, 
    #  eeg_dataset_fullì´ ë£¨í”„ ë°–ì— ìˆë‹¤ë©´ ì´ë“¤ì€ ì‚­ì œí•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
    #  ë§Œì•½ eeg_dataset_fullë„ ë£¨í”„ ì•ˆì—ì„œ ë§¤ë²ˆ ë¡œë“œí•œë‹¤ë©´ ì‚­ì œ ëŒ€ìƒì…ë‹ˆë‹¤.)

    # 2. GPU ìºì‹œ ë¹„ìš°ê¸° (PyTorch)
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("GPU ìºì‹œë¥¼ ë¹„ì› ìŠµë‹ˆë‹¤.")

    # 3. íŒŒì´ì¬ ê°€ë¹„ì§€ ì»¬ë ‰í„° ëª…ì‹œì  í˜¸ì¶œ
    collected_count = gc.collect()
    print(f"ê°€ë¹„ì§€ ì»¬ë ‰í„°ê°€ {collected_count}ê°œì˜ ê°ì²´ë¥¼ ìˆ˜ê±°í–ˆìŠµë‹ˆë‹¤.")
    # --- ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ ---

combo_pbar.close()
print(f"\nğŸ‰ ì „ì²´ ê·¸ë¦¬ë“œ ì„œì¹˜ ì™„ë£Œ! ìµœì¢… ë² ìŠ¤íŠ¸ ê²€ì¦ ì†ì‹¤: {overall_best_val_loss:.4f}")
