import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from transformers import AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig, AutoTokenizer # BitsAndBytesConfig ì¶”ê°€
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training,PeftModel ,TaskType # peft ê´€ë ¨ ëª¨ë“ˆ ì¶”ê°€
import pandas as pd
import shutil
import os
import numpy as np
from torch.nn import CrossEntropyLoss
from torch.utils.data import random_split
import random
import itertools
import gc
import nltk
from rouge_score import rouge_scorer
import jiwer
from sklearn.model_selection import train_test_split
import time
from tqdm import tqdm
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
import math
from pathlib import Path
import csv
import json

os.environ["CUDA_VISIBLE_DEVICES"] = "1"
print("Available devices:", torch.cuda.device_count())  # 1ê°œë§Œ ë³´ì—¬ì•¼ ì •ìƒ
print("Device name:", torch.cuda.get_device_name(0))    # ì‹¤ì œë¡œ GPU 1ë²ˆ ì´ë¦„ì´ ë‚˜ì˜´

SEED = 42

def set_seeds(seed_value):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)  # ì—¬ëŸ¬ GPU ì‚¬ìš© ì‹œ
        # CUDA ì—°ì‚°ì˜ ê²°ì •ë¡ ì  ì‹¤í–‰ ì„¤ì • (ì„±ëŠ¥ì— ì•½ê°„ ì˜í–¥ ì¤„ ìˆ˜ ìˆìŒ)
        # torch.backends.cudnn.deterministic = True
        # torch.backends.cudnn.benchmark = False

set_seeds(SEED)
print(f"ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‹œë“œê°€ {SEED}ë¡œ ê³ ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

class EEGDataset(Dataset):
    def __init__(self, data_dir="/home/work/skku/hyo/hyo/dataset/word-sentence.parquet"):
        df = pd.read_parquet(data_dir)
        raw = df["eeg"].tolist()
        print(f"[1] ë¡œë“œëœ ìƒ˜í”Œ ê°œìˆ˜: {len(raw)}")

        word_arrays = []
        for idx, x in enumerate(raw):
            # ë¬¸ì¥ EEGë§Œ ìˆëŠ” ê²½ìš°ì—ë„ ë§ˆì§€ë§‰ ìš”ì†Œ í¬í•¨
            vecs = x[:-1] if len(x) > 1 else x
            print(f"  ìƒ˜í”Œ {idx} - ì›ë³¸ ê¸¸ì´: {len(x)}, ì‚¬ìš©í•  word-level EEG ê°œìˆ˜: {len(vecs)}")

            words = []
            for v in vecs:
                arr = np.asarray(v, dtype=np.float32).ravel()
                assert arr.ndim == 1, f"ë²¡í„° ì°¨ì› ì˜¤ë¥˜: {arr.shape}"
                words.append(arr)
            stacked = np.stack(words, axis=0)  # (Ki, 840)
            print(f"    â†’ ìŠ¤íƒ í›„ shape: {stacked.shape}")
            word_arrays.append(stacked)

        # ì „ì²´ ë‹¨ì–´ ìˆ˜ ì§‘ê³„
        all_words = np.vstack(word_arrays)   # (ì´_ë‹¨ì–´_ìˆ˜, 840)
        print(f"[2] all_words ìŠ¤íƒ shape: {all_words.shape}")

        all_words = np.nan_to_num(
            all_words, nan=0.0, posinf=0.0, neginf=0.0
        )
        mu  = all_words.mean(axis=0, keepdims=True)    # (1, 840)
        std = all_words.std(axis=0,  keepdims=True) + 1e-8
        print(f"[3] mu shape: {mu.shape}, std shape: {std.shape}")

        # ì •ê·œí™” ë° Torch ë³€í™˜
        self.eeg_seqs = []
        for idx, arr in enumerate(word_arrays):
            normed = (arr - mu) / std                  # (Ki, 840)
            print(f"  ìƒ˜í”Œ {idx} ì •ê·œí™” í›„ shape: {normed.shape}")
            self.eeg_seqs.append(torch.from_numpy(normed))

        # ë§ˆì§€ë§‰ìœ¼ë¡œ í…ìŠ¤íŠ¸
        self.texts = [
            toks[-1] if isinstance(toks, (list, np.ndarray)) else toks
            for toks in df["text"].tolist()
        ]
        print(f"[4] text ìƒ˜í”Œ ê°œìˆ˜: {len(self.texts)}")

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.eeg_seqs[idx], self.texts[idx]

class ConvEEGEncoder(nn.Module):
    """
    840-dim ë²¡í„°ë¥¼ 1Ã—840 ì‹œí€€ìŠ¤ë¡œ ë³´ê³  Conv1D ë‘ ì¸µìœ¼ë¡œ ì ì¬í‘œí˜„ ìƒì„±
    ì¶œë ¥ì€ (B, latent_dim)
    """
    def __init__(self, input_dim=840, latent_dim=128, hidden=256):
        super().__init__()
        self.conv_stack = nn.Sequential(
            nn.Conv1d(1, hidden, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv1d(hidden, latent_dim, kernel_size=3, padding=1), nn.ReLU()
        )
        self.pool = nn.AdaptiveAvgPool1d(1)   # ê¸¸ì´ 840 â†’ 1 ë¡œ ì••ì¶•

    def forward(self, x):           # x: (B, feat)
        x = x.unsqueeze(1)          # (B, 1, 840)
        z = self.conv_stack(x)      # (B, latent_dim, 840)
        z = self.pool(z).squeeze(-1)  # (B, latent_dim)
        return z

class RVQ(nn.Module):
    def __init__(self, num_quantizers, num_embeddings, embedding_dim, commitment_cost=0.25):
        super().__init__()
        self.num_quantizers = num_quantizers # ì½”ë“œë¶ì˜ ê°œìˆ˜ (n_q)
        self.num_embeddings = num_embeddings # ê° ì½”ë“œë¶ ë‚´ ì„ë² ë”©(ì½”ë“œì›Œë“œ) ê°œìˆ˜ (n_emb, ì–´íœ˜ í¬ê¸°)
        self.embedding_dim = embedding_dim   # ê° ì„ë² ë”©ì˜ ì°¨ì› (D, latent_dimê³¼ ë™ì¼)
        self.commitment_cost = commitment_cost # VQ ì†ì‹¤ ê³„ì‚° ì‹œ ì‚¬ìš©ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°

        # num_quantizers ê°œìˆ˜ë§Œí¼ì˜ ì½”ë“œë¶(nn.Embedding ë ˆì´ì–´)ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì§
        self.codebooks = nn.ModuleList([
            nn.Embedding(self.num_embeddings, self.embedding_dim) for _ in range(self.num_quantizers)
        ])
        # ì½”ë“œë¶ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜í–‰)
        for i, codebook in enumerate(self.codebooks):
            nn.init.uniform_(codebook.weight, -1.0 / self.num_embeddings, 1.0 / self.num_embeddings)

    def forward(self, z_e): # ì…ë ¥ z_eì˜ ëª¨ì–‘: (B, L, D), ì—¬ê¸°ì„œ L=1, D=embedding_dim
        B, L, D = z_e.shape
        z_e_flat = z_e.reshape(-1, D) # (B*L, D) ëª¨ì–‘ìœ¼ë¡œ í¼ì¹¨ (ì—¬ê¸°ì„œëŠ” (B, D)ì™€ ë™ì¼)

        all_quantized_stages = [] # ê° ì½”ë“œë¶ì—ì„œ ì–‘ìí™”ëœ ë²¡í„°ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        all_indices = []          # ê° ì½”ë“œë¶ì—ì„œ ì„ íƒëœ ì¸ë±ìŠ¤ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        residual = z_e_flat       # ì²« ë²ˆì§¸ ì½”ë“œë¶ì— ì…ë ¥ë  ì”ì°¨ (ì´ˆê¸°ì—ëŠ” z_e_flat ì „ì²´)

        # num_quantizers ë§Œí¼ ë°˜ë³µ (ê° ì½”ë“œë¶ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬)
        for i in range(self.num_quantizers):
            codebook = self.codebooks[i] # í˜„ì¬ ì‚¬ìš©í•  ì½”ë“œë¶

            # í˜„ì¬ ì”ì°¨(residual)ì™€ í˜„ì¬ ì½”ë“œë¶ì˜ ëª¨ë“  ì„ë² ë”© ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì œê³± ê³„ì‚°
            # distances ëª¨ì–‘: (B*L, num_embeddings)
            distances = torch.sum(residual**2, dim=1, keepdim=True) \
                        - 2 * torch.matmul(residual, codebook.weight.t()) \
                        + torch.sum(codebook.weight**2, dim=1, keepdim=True).t()

            # ê°€ì¥ ê°€ê¹Œìš´ ì„ë² ë”©ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            # current_indices ëª¨ì–‘: (B*L)
            current_indices = torch.argmin(distances, dim=1)
            all_indices.append(current_indices) # í˜„ì¬ ì½”ë“œë¶ì˜ ì¸ë±ìŠ¤ ì €ì¥

            # ì„ íƒëœ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–‘ìí™”ëœ ë²¡í„°(ì½”ë“œì›Œë“œ) ê°€ì ¸ì˜¤ê¸°
            # quantized_vector ëª¨ì–‘: (B*L, D)
            quantized_vector = codebook(current_indices)
            # ì›ë˜ ëª¨ì–‘ (B, L, D)ë¡œ ë³µì›í•˜ì—¬ ì €ì¥ (ì—¬ê¸°ì„œëŠ” (B, 1, D))
            all_quantized_stages.append(quantized_vector.reshape(B, L, D))

            # ë‹¤ìŒ ì½”ë“œë¶ìœ¼ë¡œ ë„˜ê¸¸ ì”ì°¨ ê³„ì‚°
            # ì¤‘ìš”: quantized_vectorì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ì„ ëŠê¸° ìœ„í•´ .detach() ì‚¬ìš©
            residual = residual - quantized_vector.detach()

        # ëª¨ë“  ì½”ë“œë¶ì—ì„œ ë‚˜ì˜¨ ì–‘ìí™”ëœ ë²¡í„°ë“¤ì„ í•©ì‚° (EEGTran ë…¼ë¬¸ Figure 2 ì°¸ì¡°)
        # final_quantized_output ëª¨ì–‘: (B, L, D)
        final_quantized_output = torch.stack(all_quantized_stages, dim=0).sum(dim=0)

        # ìˆ˜ì§‘ëœ ì¸ë±ìŠ¤ë“¤ì„ (B, L, num_quantizers) í˜•íƒœë¡œ ìŒ“ìŒ
        # stacked_indices ëª¨ì–‘: (B, L, n_q) (ì—¬ê¸°ì„œëŠ” (B, 1, n_q))
        stacked_indices = torch.stack(all_indices, dim=1).reshape(B, L, self.num_quantizers)

        # ìµœì¢… ë°˜í™˜ê°’: í•©ì‚°ëœ ì–‘ìí™” ë²¡í„°, ìŒ“ì¸ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤, VQ ì†ì‹¤
        # RVQTokenizerì˜ forwardì—ì„œëŠ” ì´ ì¤‘ ì²« ë‘ ê°œë¥¼ zq, indicesë¡œ ë°›ê²Œ ë©ë‹ˆë‹¤.
        return final_quantized_output, stacked_indices

class RVQTokenizer(nn.Module):
    def __init__(self,
                 feat=840,
                 latent=128,  # 1024->2048
                 n_q=12,
                 n_emb=512,
                 hidden=256,
                 TOKENIZER_CHECKPOINT_PATH = "/home/work/skku/hyo/hyo/model/rvq_best_model_word_12_512.pt"
                 ):
        super().__init__()
        self.n_q = n_q
        self.n_emb = n_emb
        # ì‹¤ì œ ConvEEGEncoderì™€ RVQ ëª¨ë“ˆì´ ì—¬ê¸°ì— ì™€ì•¼ í•¨
        self.enc = ConvEEGEncoder(feat, latent, hidden)
        self.rvq = RVQ(num_quantizers=n_q, num_embeddings=n_emb, embedding_dim=latent)

        checkpoint = torch.load(TOKENIZER_CHECKPOINT_PATH, map_location="cpu")
        self.enc.load_state_dict(checkpoint["encoder"])
        for i, cb_weight_tensor in enumerate(checkpoint["codebooks"]):
            self.rvq.codebooks[i].weight.data = cb_weight_tensor

    @torch.no_grad()
    def forward(self, x): # x: (B, 840)
        z = self.enc(x)
        quantized_vector, token_indices = self.rvq(z.unsqueeze(1)) # vq_lossëŠ” ë¬´ì‹œ
        zq = quantized_vector
        indices = token_indices # ëª¨ì–‘ (B, 1, n_q)
        # ë§Œì•½ LLaDA ì…ë ¥ìš©ìœ¼ë¡œ (B, n_q) ëª¨ì–‘ì˜ ì¸ë±ìŠ¤ë¥¼ ì›í•œë‹¤ë©´ squeeze(1) í•„ìš”
        # return zq, indices.squeeze(1)
        return zq, indices # í˜„ì¬ pasted_content.txtì˜ ì£¼ì„ê³¼ ë§ì¶”ë ¤ë©´ ì´ëŒ€ë¡œ

class UnifiedEEGTextTokenizer:
    def __init__(self,
                rvq_tokenizer_instance,
                llada_text_tokenizer_instance,
                max_seq_length,
                v_text_original,
                eeg_token_length,
                ):

        self.rvq_tokenizer = rvq_tokenizer_instance
        self.llada_text_tokenizer = llada_text_tokenizer_instance
        self.max_seq_length = max_seq_length
        self.v_text_original = v_text_original
        self.eeg_token_length = eeg_token_length


        self.bos_token_id = torch.tensor([self.llada_text_tokenizer.bos_token_id], dtype=torch.long, device=config.system.DEVICE)
        self.eos_token_id = torch.tensor([self.llada_text_tokenizer.eos_token_id], dtype=torch.long, device=config.system.DEVICE)
        self.pad_token_id = self.llada_text_tokenizer.pad_token_id if self.llada_text_tokenizer.pad_token_id is not None else self.llada_text_tokenizer.eos_token_id

        self.user_prompt_intro_ids = self.llada_text_tokenizer.encode(
                "<start_id>user<end_id>\n",
                add_special_tokens=False,
                return_tensors="pt"
            ).squeeze(0).to(config.system.DEVICE)

        self.assistant_prompt_intro_ids = self.llada_text_tokenizer.encode(
                "<eot_id><start_id>assistant<end_id>\n",
                add_special_tokens=False,
                return_tensors="pt"
            ).squeeze(0).to(config.system.DEVICE)

        print(f"Unified Tokenizer Initialized:")
        print(f"  BOS ID: {self.bos_token_id.item()}")
        print(f"  EOS ID: {self.eos_token_id.item()}")
        print(f"  PAD ID: {self.pad_token_id}")
        print(f"  User Prompt Intro IDs ({len(self.user_prompt_intro_ids)} tokens): {self.user_prompt_intro_ids.tolist()}")
        print(f"  Assistant Prompt Intro IDs ({len(self.assistant_prompt_intro_ids)} tokens): {self.assistant_prompt_intro_ids.tolist()}")

    def process_single_sample(self, eeg_seq, assistant_response_text):
        # eeg_seq: (N_words, 840)
        device = config.system.DEVICE

        # 1) ê° word EEG ë²¡í„° â†’ RVQTokenizer â†’ (1,1,n_q) indices â†’ (n_q,) tensor
        local_indices = []
        with torch.no_grad():
            for w in eeg_seq.to(device):                          # w: (840,)
                _, idx = self.rvq_tokenizer(w.unsqueeze(0))       # idx.shape = (1,1,n_q)
                local_indices.append(idx.squeeze(0).squeeze(0))    # (n_q,)
        # ìµœì¢… shape: (N_words * n_q,)
        local_indices = torch.cat(local_indices, dim=0)

        # 2) global token IDs: ê¸°ì¡´ text vocab_size offset
        global_eeg_ids = (local_indices + self.v_text_original).to(device)  # LongTensor

        # 3) prompt ê¸¸ì´ ê³ ì • í† í° ìˆ˜ ê³„ì‚°
        fixed_len = (
            len(self.bos_token_id)
          + len(self.user_prompt_intro_ids)
          + len(self.assistant_prompt_intro_ids)
          + len(self.eos_token_id)
        )
        # ë‚¨ëŠ” ê³µê°„
        max_resp = self.max_seq_length - fixed_len - global_eeg_ids.numel()
        if max_resp < 1:
            # EEG í† í°ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ…ë‹ˆë‹¤
            global_eeg_ids = global_eeg_ids[: self.max_seq_length - fixed_len - 1]
            max_resp = 1

        # 4) ì–´ì‹œìŠ¤í„´íŠ¸ í…ìŠ¤íŠ¸ í† í°í™”
        tok = self.llada_text_tokenizer(
            assistant_response_text,
            max_length=max_resp,
            truncation=True,
            add_special_tokens=False,
            return_tensors="pt"
        )
        assistant_ids = tok.input_ids.squeeze(0).to(device)

        # 5) input_ids, labels, attention_mask êµ¬ì„±
        input_ids = torch.cat([
            self.bos_token_id,
            self.user_prompt_intro_ids,
            global_eeg_ids,
            self.assistant_prompt_intro_ids,
            assistant_ids,
            self.eos_token_id
        ], dim=0)

        prompt_len = (
            len(self.bos_token_id)
          + len(self.user_prompt_intro_ids)
          + global_eeg_ids.numel()
          + len(self.assistant_prompt_intro_ids)
        )

        # labels: prompt ìœ„ì¹˜ëŠ” -100
        labels = input_ids.clone()
        labels[:prompt_len] = -100

        # padding / truncate
        cur_len = input_ids.numel()
        if cur_len < self.max_seq_length:
            pad_len = self.max_seq_length - cur_len
            pad = torch.full((pad_len,), self.pad_token_id, dtype=torch.long, device=device)
            input_ids    = torch.cat([input_ids, pad], dim=0)
            labels       = torch.cat([labels, torch.full((pad_len,), -100, dtype=torch.long, device=device)], dim=0)
            attn_mask    = torch.cat([torch.ones(cur_len, dtype=torch.long, device=device),
                                      torch.zeros(pad_len, dtype=torch.long, device=device)], dim=0)
        else:
            input_ids    = input_ids[:self.max_seq_length]
            labels       = labels[:self.max_seq_length]
            attn_mask    = torch.ones(self.max_seq_length, dtype=torch.long, device=device)

        return {
            "input_ids": input_ids,
            "attention_mask": attn_mask,
            "labels": labels,
            "prompt_lengths": torch.tensor(prompt_len, dtype=torch.long, device=device)
        }

    def build_chat_template_prompt(self, eeg_seq):
        device = config.system.DEVICE
        # â‘  RVQ â†’ indices
        local_indices = []
        with torch.no_grad():
            for w in eeg_seq.to(device):
                _, idx = self.rvq_tokenizer(w.unsqueeze(0))
                local_indices.append(idx.squeeze(0).squeeze(0))
        global_eeg_ids = (torch.cat(local_indices, dim=0) + self.v_text_original).to(device)

        input_ids = torch.cat([
            self.bos_token_id,
            self.user_prompt_intro_ids,
            global_eeg_ids,
            self.assistant_prompt_intro_ids
        ], dim=0)

        return {
            "input_ids": input_ids.unsqueeze(0),
            "attention_mask": torch.ones_like(input_ids).unsqueeze(0),
            "prompt_len": input_ids.numel()
        }

class DataCollatorForEEGTextSFT:
    def __init__(self, unified_tokenizer_instance):
        self.unified_tokenizer = unified_tokenizer_instance

    def __call__(self, batch_of_samples):
        processed_samples = []
        for eeg_tensor, assistant_response_text in batch_of_samples:
            if eeg_tensor is None or assistant_response_text is None:
                continue
            processed_samples.append(self.unified_tokenizer.process_single_sample(eeg_tensor, assistant_response_text))

        if not processed_samples:
            # print("Warning: Collator received no valid samples to batch.")
            # ë¹ˆ í…ì„œë¥¼ ë°˜í™˜í•˜ê±°ë‚˜ Noneì„ ë°˜í™˜í•˜ì—¬ í•™ìŠµ ë£¨í”„ì—ì„œ ì²˜ë¦¬
            return None

        batched_input_ids = torch.stack([s["input_ids"] for s in processed_samples])
        batched_attention_mask = torch.stack([s["attention_mask"] for s in processed_samples])
        batched_labels = torch.stack([s["labels"] for s in processed_samples])
        batched_prompt_lengths = torch.stack([s["prompt_lengths"] for s in processed_samples])

        return {
            "input_ids": batched_input_ids,
            "attention_mask": batched_attention_mask,
            "labels": batched_labels,
            "prompt_lengths": batched_prompt_lengths
        }

def collate_fn_for_evaluation(batch):
    eeg_data_list = [item[0] for item in batch] # ì›ë³¸ EEG ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    reference_texts_list = [item[1] for item in batch] # ì°¸ì¡° í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

    # EEG ë°ì´í„°ëŠ” ë°°ì¹˜ ë‚´ì—ì„œ íŒ¨ë”© ì—†ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ìœ ì§€í•˜ê±°ë‚˜,
    # ë§Œì•½ ëª¨ë“  EEG ë°ì´í„°ì˜ ê¸¸ì´ê°€ ê°™ë‹¤ë©´ torch.stackì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•˜ê³ , ìƒì„± ë£¨í”„ì—ì„œ ê°œë³„ ì²˜ë¦¬í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    return {
        "batched_eeg_data": eeg_data_list, 
        "batched_reference_texts": reference_texts_list
    }

import numpy as np
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def calculate_bleu_scores(references, hypotheses):
    """
    references: List[List[List[str]]]  # ê° ìƒ˜í”Œë§ˆë‹¤ ì—¬ëŸ¬ ë ˆí¼ëŸ°ìŠ¤ ë¬¸ì¥(í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸)
    hypotheses: List[List[str]]        # ê° ìƒ˜í”Œë§ˆë‹¤ ìƒì„±ëœ ë¬¸ì¥(í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸)
    
    ë°˜í™˜: {
        "BLEU-1": float,
        "BLEU-2": float,
        "BLEU-3": float,
        "BLEU-4": float
    }
    """
    # ìŠ¤ë¬´ë”© í•¨ìˆ˜
    smooth_fn = SmoothingFunction().method1
    
    # ì¸¡ì •í•  BLEU weight ì„¤ì •
    weight_dict = {
        "BLEU-1": (1.0, 0.0, 0.0, 0.0),
        "BLEU-2": (0.5, 0.5, 0.0, 0.0),
        "BLEU-3": (1/3, 1/3, 1/3, 0.0),
        "BLEU-4": (0.25, 0.25, 0.25, 0.25),
    }
    
    results = {}
    for name, weights in weight_dict.items():
        scores = []
        for refs, hyp in zip(references, hypotheses):
            # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ë ˆí¼ëŸ°ìŠ¤ê°€ ì—†ìœ¼ë©´ 0ì  ì²˜ë¦¬
            if not refs or not refs[0] or not hyp:
                scores.append(0.0)
                continue
            score = sentence_bleu(
                refs,
                hyp,
                weights=weights,
                smoothing_function=smooth_fn
            )
            scores.append(score)
        # ìƒ˜í”Œë³„ ì ìˆ˜ í‰ê· 
        results[name] = float(np.mean(scores)) if scores else 0.0
    
    return results

def calculate_rouge_scores(references, hypotheses):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    all_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}

    for ref, hyp in zip(references, hypotheses):
        if not ref or not hyp:
            # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬ ë˜ëŠ” íŠ¹ì • ê°’ í• ë‹¹
            for key in all_scores:
                all_scores[key].append(0.0) # f-measure ê¸°ì¤€ 0ì ìœ¼ë¡œ ì²˜ë¦¬
            continue
        scores = scorer.score(ref, hyp)
        all_scores['rouge1'].append(scores['rouge1'].fmeasure)
        all_scores['rouge2'].append(scores['rouge2'].fmeasure)
        all_scores['rougeL'].append(scores['rougeL'].fmeasure)

    avg_scores = {key: np.mean(values) if values else 0.0 for key, values in all_scores.items()}
    return avg_scores

def calculate_wer(references, hypotheses):
    """
    ì£¼ì–´ì§„ ì •ë‹µê³¼ ì˜ˆì¸¡ì— ëŒ€í•´ WERì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    references: list of strings (ê° ìƒ˜í”Œì— ëŒ€í•œ ì •ë‹µ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸)
    hypotheses: list of strings (ê° ìƒ˜í”Œì— ëŒ€í•œ ì˜ˆì¸¡ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸)
    """
    if not references or not hypotheses or len(references) != len(hypotheses):
        return 1.0 # ì˜ëª»ëœ ì…ë ¥ ì²˜ë¦¬, WERì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ 1.0 (100% ì˜¤ë¥˜) ë°˜í™˜

    # jiwer.compute_measuresëŠ” ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ì§‘ê³„ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    # ê°œë³„ ë¬¸ì¥ì— ëŒ€í•œ WERì„ ê³„ì‚°í•˜ê³  í‰ê· ë‚´ëŠ” ê²ƒë³´ë‹¤, ì „ì²´ ë§ë­‰ì¹˜ì— ëŒ€í•œ WERì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
    measures = jiwer.compute_measures(references, hypotheses)
    return measures['wer']

def add_gumbel_noise(logits, temperature):
    '''
    The Gumbel max is a method for sampling categorical distributions.
    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.
    Thus, we use float64.
    '''
    if temperature == 0:
        return logits
    logits = logits.to(torch.float64)
    noise = torch.rand_like(logits, dtype=torch.float64)
    gumbel_noise = (- torch.log(noise)) ** temperature
    return logits.exp() / gumbel_noise


def get_num_transfer_tokens(mask_index, steps):
    '''
    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.
    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),
    the expected number of tokens transitioned at each step should be consistent.

    This function is designed to precompute the number of tokens that need to be transitioned at each step.
    '''
    mask_num = mask_index.sum(dim=1, keepdim=True)

    base = mask_num // steps
    remainder = mask_num % steps

    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base

    for i in range(mask_num.size(0)):
        num_transfer_tokens[i, :remainder[i]] += 1

    return num_transfer_tokens

@ torch.no_grad()
def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,
             cfg_scale=0., remasking='low_confidence', mask_id=126336):
    '''
    Args:
        model: Mask predictor.
        prompt: A tensor of shape (1, L).
        steps: Sampling steps, less than or equal to gen_length.
        gen_length: Generated answer length.
        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
        temperature: Categorical distribution sampling temperature.
        cfg_scale: Unsupervised classifier-free guidance scale.
        remasking: Remasking strategy. 'low_confidence' or 'random'.
        mask_id: The toke id of [MASK] is 126336.
    '''
    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    prompt_index = (x != mask_id)

    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length

    assert steps % num_blocks == 0
    steps = steps // num_blocks

    for num_block in range(num_blocks):
        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)
        for i in range(steps):
            mask_index = (x == mask_id)
            if cfg_scale > 0.:
                un_x = x.clone()
                un_x[prompt_index] = mask_id
                x_ = torch.cat([x, un_x], dim=0)
                logits = model(x_).logits
                logits, un_logits = torch.chunk(logits, 2, dim=0)
                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)
            else:
                logits = model(x).logits

            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)
            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l

            if remasking == 'low_confidence':
                p = F.softmax(logits.to(torch.float64), dim=-1)
                x0_p = torch.squeeze(
                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l
            elif remasking == 'random':
                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)
            else:
                raise NotImplementedError(remasking)

            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf

            x0 = torch.where(mask_index, x0, x)
            confidence = torch.where(mask_index, x0_p, -np.inf)

            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)
            for j in range(confidence.shape[0]):
                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])
                transfer_index[j, select_index] = True
            x[transfer_index] = x0[transfer_index]

    return x

def load_finetuned_eeg_llada(
        base_ckpt: str = "GSAI-ML/LLaDA-8B-Base",
        comp_dir: Path = Path(
            "/home/work/skku/hyo/hyo/checkpoints_v4/best_finetuned_model_components"  # wte.pth, lm_head.pth, adapters/
        ),
        rvq_n_emb: int = 512,
        device: str = "cuda",
        load_in_4bit: bool = True,
):
    # --- â‘  base LLaDA (4-bit optional) ------------------------------
    bnb_cfg = (BitsAndBytesConfig(
        load_in_4bit=True, bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)
        if load_in_4bit else None)

    base = AutoModelForCausalLM.from_pretrained(
        base_ckpt, quantization_config=bnb_cfg, torch_dtype="auto",
        trust_remote_code=True).to(device)

    # --- â‘¡ vocab resize --------------------------------------------
    v_txt = base.config.vocab_size
    base.resize_token_embeddings(v_txt + rvq_n_emb + 1)   # +1 = EEG-MASK

    # --- â‘¢ embedding / lm_head ë³µì› -------------------------------
    wte_path = comp_dir / "wte.pth"
    lm_path = comp_dir / "lm_head.pth"

    llada_core = getattr(base, "model", base)          # 1ì°¨
    llada_core = getattr(llada_core, "model", llada_core)  # 2ì°¨
    #   3-2. transformer í•¸ë“¤
    transformer = getattr(llada_core, "transformer", llada_core)

    if wte_path.exists():
        base.get_input_embeddings().load_state_dict(torch.load(wte_path, map_location="cpu"))
        print("âœ” wte restored")

    if lm_path.exists():
        transformer.ff_out.load_state_dict(torch.load(lm_path, map_location="cpu"))
        print("âœ” lm_head (ff_out) restored")

    # --- â‘£ LoRA attach (trainable=False ë¡œ inference) --------------
    adapter_dir = comp_dir / "adapters"
    if adapter_dir.exists():
        model = PeftModel.from_pretrained(base, adapter_dir, is_trainable=False)
        print("âœ” LoRA adapter loaded")
    else:
        model = base
        print("âš  LoRA adapter not found, base model only")

    model.eval()
    return model

class SystemConfig(BaseModel):
    SEED: int = 42
    DEVICE: str = "cuda" if torch.cuda.is_available() else "cpu"
    NUM_WORKERS: int = 0

class PathsConfig(BaseModel):
    DATASET_PATH: str = "/home/work/skku/hyo/hyo/dataset/word-sentence.parquet"
    TOKENIZER_CHECKPOINT_PATH: str = "/home/work/skku/hyo/hyo/model/rvq_best_model_word_12_512.pt"
    MODEL_SAVE_DIR: str = "./saved_models"
    BEST_MODEL_FILENAME: str = "eeg_llada_sft_best_model.pth"
    # LLADA_LOSS_FUNCTION_PATH: str = "/home/ubuntu/llada_loss_function.py" # í•„ìš”ì‹œ ì¶”ê°€
    # MODIFIED_TRAINING_LOOPS_PATH: str = "/home/ubuntu/modified_training_loops.py" # í•„ìš”ì‹œ ì¶”ê°€

class EEGEncoderConfig(BaseModel):
    INPUT_DIM: int = 840
    LATENT_DIM: int = 128 # RVQì˜ embedding_dimê³¼ ì¼ì¹˜í•´ì•¼ í•¨
    HIDDEN_DIM: int = 256

class RVQConfig(BaseModel):
    NUM_QUANTIZERS: int = 12 # RVQTokenizerì˜ n_q, UnifiedEEGTextTokenizerì˜ eeg_token_lengthì™€ ì¼ì¹˜
    NUM_EMBEDDINGS: int = 512 # RVQTokenizerì˜ n_emb
    EMBEDDING_DIM: int = 128 # EEGEncoderConfigì˜ LATENT_DIMê³¼ ì¼ì¹˜
    COMMITMENT_COST: float = 0.25

class TokenizerConfig(BaseModel):
    # RVQTokenizer ë‚´ë¶€ íŒŒë¼ë¯¸í„° (EEGEncoderConfig, RVQConfig ê°’ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
    # UnifiedEEGTextTokenizer íŒŒë¼ë¯¸í„°
    MAX_SEQ_LENGTH: int = 1024 # LLaDA ëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê³ ë ¤
    V_TEXT_ORIGINAL: int = 32000 # LLaMA í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ í¬ê¸° (LLaDA-8B ê¸°ì¤€)
    # EEG_TOKEN_LENGTH: int = 12 # RVQConfig.NUM_QUANTIZERS ì™€ ë™ì¼
    LLM_MODEL_NAME: str = "GSAI-ML/LLaDA-8B-Base" # LLM í† í¬ë‚˜ì´ì € ë¡œë“œìš©

class ModelConfig(BaseModel):
    LLM_MODEL_NAME: str = "GSAI-ML/LLaDA-8B-Base"
    USE_QLORA: bool = True
    LORA_R: int = 16
    LORA_ALPHA: int = 32
    LORA_DROPOUT: float = 0.05
    LORA_BIAS: str = "none"
    # LLADA_MASK_TOKEN_ID: Optional[int] = None # ë™ì ìœ¼ë¡œ ì„¤ì •ë  ìˆ˜ ìˆìŒ (ì–´íœ˜í¬ê¸° + 1)

class TrainingConfig(BaseModel):
    BATCH_SIZE: int = 4 # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ
    NUM_EPOCHS: int = 10
    START_EPOCH: int = 0
    LEARNING_RATE: float = 1e-4
    GRADIENT_ACCUMULATION_STEPS: int = 4 # BATCH_SIZE * GRAD_ACCUM = ì‹¤ì œ ë°°ì¹˜ í¬ê¸°
    MAX_GRAD_NORM: float = 1.0
    # SCHEDULER: Optional[str] = None # ì˜ˆ: "StepLR"
    TRAIN_LOG_INTERVAL: int = 1
    PATIENCE_EARLY_STOPPING: int = 3 # 0ì´ë©´ ë¹„í™œì„±í™”

class GenerationConfig(BaseModel):
    RUN_TEST_LOOP_EACH_EPOCH: bool = False
    USE_LLADA_SAMPLING_FOR_GENERATION: bool = True
    MAX_GEN_TOKENS: int = 64
    NUM_SAMPLING_STEPS_GEN: int = 10
    REMASKING_STRATEGY_GEN: str = "low_confidence"
    REMASKING_RATIO_GEN: float = 0.25
    TEMPERATURE_GEN: float = 0.7
    TOP_K_GEN: int = 50
    TOP_P_GEN: float = 0.9
    HF_NUM_BEAMS_GEN: int = 1
    # HF_MAX_LENGTH_GEN: Optional[int] = None # ë™ì ìœ¼ë¡œ ì„¤ì • (ì…ë ¥ê¸¸ì´ + MAX_GEN_TOKENS)

class ExperimentConfig(BaseModel):
    system: SystemConfig = Field(default_factory=SystemConfig)
    paths: PathsConfig = Field(default_factory=PathsConfig)
    eeg_encoder: EEGEncoderConfig = Field(default_factory=EEGEncoderConfig)
    rvq: RVQConfig = Field(default_factory=RVQConfig)
    tokenizer: TokenizerConfig = Field(default_factory=TokenizerConfig)
    model: ModelConfig = Field(default_factory=ModelConfig)
    training: TrainingConfig = Field(default_factory=TrainingConfig)
    generation: GenerationConfig = Field(default_factory=GenerationConfig)

    # LLADA_MASK_TOKEN_IDëŠ” ë™ì ìœ¼ë¡œ ì„¤ì •ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ˆê¸°í™” í›„ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥
    # ì˜ˆ: config.model.LLADA_MASK_TOKEN_ID = tokenizer.llada_text_tokenizer.vocab_size + 1

config = ExperimentConfig()

comp_dir = Path("/home/work/skku/hyo/hyo/checkpoints_v4/best_finetuned_model_components")
model = load_finetuned_eeg_llada(
            base_ckpt="GSAI-ML/LLaDA-8B-Base",
            comp_dir=comp_dir,
            rvq_n_emb=512,
            device="cuda")

llada_txt_tokenizer = AutoTokenizer.from_pretrained(config.model.LLM_MODEL_NAME)
rvq_eeg_tokenizer = RVQTokenizer()
rvq_eeg_tokenizer = rvq_eeg_tokenizer.to(config.system.DEVICE)
rvq_eeg_tokenizer.eval() # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •

v_original = llada_txt_tokenizer.vocab_size
eeg_seq_len = 12 # RVQ_N_Q
model_max_len = 512

unified_eeg_text_tokenizer = UnifiedEEGTextTokenizer(
    rvq_tokenizer_instance=rvq_eeg_tokenizer,
    llada_text_tokenizer_instance=llada_txt_tokenizer,
    max_seq_length=model_max_len,
    v_text_original=v_original,
    eeg_token_length=eeg_seq_len
)

# 1. EEGDataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (config ì‚¬ìš©)
# EEGDataset í´ë˜ìŠ¤ ì •ì˜ëŠ” ì´ë¯¸ ë…¸íŠ¸ë¶ì— ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
eeg_dataset = EEGDataset(data_dir=config.paths.DATASET_PATH)
eeg_dataset = eeg_dataset # í…ŒìŠ¤íŠ¸ìš©
num_total_samples = len(eeg_dataset)
indices = list(range(num_total_samples))

# 2. ë°ì´í„°ì…‹ ë¶„í•  (config ì‚¬ìš©)
# ì°¸ê³ : test_size ê°’ë“¤(í˜„ì¬ 0.2 ë° 0.5)ë„ config ê°ì²´ì— ì¶”ê°€í•˜ì—¬ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì˜ˆ: config.training.TRAIN_VAL_SPLIT_RATIO, config.training.VAL_TEST_SPLIT_RATIO
train_indices, temp_test_indices = train_test_split(
    indices,
    test_size=0.2, # ì „ì²´ ë°ì´í„° ì¤‘ 20%ë¥¼ (ê²€ì¦+í…ŒìŠ¤íŠ¸)ìš©ìœ¼ë¡œ ë¶„ë¦¬
    random_state=config.system.SEED, # configì—ì„œ SEED ê°’ ì‚¬ìš©
    shuffle=True
)

val_indices, test_indices = train_test_split(
    temp_test_indices,
    test_size=0.5, # (ê²€ì¦+í…ŒìŠ¤íŠ¸)ìš© ë°ì´í„° ì¤‘ 50%ë¥¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„ë¦¬ (ì¦‰, ì „ì²´ì˜ 10%)
    random_state=config.system.SEED, # configì—ì„œ SEED ê°’ ì‚¬ìš©
    shuffle=True
)

# ê° Subset ìƒì„±
train_dataset = Subset(eeg_dataset, train_indices)
val_dataset = Subset(eeg_dataset, val_indices)
test_dataset = Subset(eeg_dataset, test_indices)

print(f"ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°: {num_total_samples}")
print(f"í•™ìŠµ ì„¸íŠ¸ í¬ê¸°: {len(train_dataset)} (ì „ì²´ì˜ {len(train_dataset)/num_total_samples:.2%})")
print(f"ê²€ì¦ ì„¸íŠ¸ í¬ê¸°: {len(val_dataset)} (ì „ì²´ì˜ {len(val_dataset)/num_total_samples:.2%})")
print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í¬ê¸°: {len(test_dataset)} (ì „ì²´ì˜ {len(test_dataset)/num_total_samples:.2%})")

# DataCollatorForEEGTextSFT ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì´ì „ì— data_collator ë¡œ ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •)
data_collator = DataCollatorForEEGTextSFT(unified_eeg_text_tokenizer)

# 3. DataLoader ìƒì„± (config ì‚¬ìš©)
# í•™ìŠµ ë°ì´í„° ë¡œë”
train_dataloader = DataLoader(
    train_dataset,
    batch_size=config.training.BATCH_SIZE, # configì—ì„œ BATCH_SIZE ê°’ ì‚¬ìš©
    collate_fn=collate_fn_for_evaluation,
    shuffle=True,
    num_workers=config.system.NUM_WORKERS, # configì—ì„œ NUM_WORKERS ê°’ ì‚¬ìš©
    pin_memory= False
)

# ê²€ì¦ ë°ì´í„° ë¡œë”
val_dataloader = DataLoader(
    val_dataset,
    batch_size=config.training.BATCH_SIZE, # configì—ì„œ BATCH_SIZE ê°’ ì‚¬ìš©
    collate_fn=collate_fn_for_evaluation,
    shuffle=False,
    num_workers=config.system.NUM_WORKERS, # configì—ì„œ NUM_WORKERS ê°’ ì‚¬ìš©
    pin_memory= False
)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
test_dataloader = DataLoader(
    test_dataset,
    batch_size=config.training.BATCH_SIZE, # configì—ì„œ BATCH_SIZE ê°’ ì‚¬ìš©
    collate_fn=collate_fn_for_evaluation,
    shuffle=False,
    num_workers=config.system.NUM_WORKERS, # configì—ì„œ NUM_WORKERS ê°’ ì‚¬ìš©
    pin_memory= False
)

print(f"\ní•™ìŠµ ë°ì´í„°ë¡œë” ë°°ì¹˜ ìˆ˜: {len(train_dataloader)}")
print(f"ê²€ì¦ ë°ì´í„°ë¡œë” ë°°ì¹˜ ìˆ˜: {len(val_dataloader)}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ë°°ì¹˜ ìˆ˜: {len(test_dataloader)}")


print("\në°ì´í„° ë¡œë”© ë° ë¶„í•  (config ê¸°ë°˜) ì™„ë£Œ.")

# â”€â”€â”€â”€â”€ ìƒ˜í”Œë§ í•˜ì´í¼íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€
GEN_STEPS   = 64    # diffusion ìŠ¤í…
GEN_LEN     = 128        # ìƒì„± í† í° ìˆ˜
BLOCK_LEN   = 64
TEMP        = 0.3
CFG_SCALE   = 0.9
REMASKING   = "low_confidence"

device = next(model.parameters()).device        # ëª¨ë¸ì´ ì˜¬ë¼ê°„ GPU/CPU

model.eval()
all_hyp, all_ref = [], []

# â”€â”€â”€ ê²°ê³¼ ì €ì¥ìš© â”€â”€â”€
out_dir      = Path("/home/work/skku/hyo/hyo/generate")
out_dir.mkdir(exist_ok=True)
csv_path     = out_dir / "eeg_gen_results.csv"
metrics_path = out_dir / "final_metrics.json"

with torch.no_grad():
    for batch in tqdm(test_dataloader, desc="ğŸ”®  Generating from EEG"):
        eeg_list = batch["batched_eeg_data"]            # list[Tensor(840)]
        ref_list = batch["batched_reference_texts"]     # list[str]

        for eeg_tensor, ref_text in zip(eeg_list, ref_list):
            # 1) EEG  âœ  RVQ  âœ  prompt
            prompt_pack = unified_eeg_text_tokenizer.build_chat_template_prompt(eeg_tensor)
            prompt_ids  = prompt_pack["input_ids"].to(device)          # (1, T)
            prompt_len  = prompt_pack["prompt_len"]
            # EEG í† í° id ê°€ ì„œë¡œ ë‹¤ë¥¸ì§€
            # print(prompt_ids[0, :20])          # ë‹¤ë¥¸ EEG sample ì—ì„œ ê°’ì´ ë‹¬ë¼ì•¼ í•¨

            # # ì–´ëŒ‘í„° ë¡œë“œëëŠ”ì§€
            # print(model.peft_config.keys())    # LoRA ì„¤ì • dict ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ì–´ëŒ‘í„° ë¯¸ì ìš©

            # 2) LLaDA diffusion sampling
            out_ids = generate(
                model,                                  # â† ë°”ë¡œ model ì „ë‹¬
                prompt = prompt_ids,         # (T,)
                steps  = GEN_STEPS,
                gen_length = GEN_LEN,
                block_length = BLOCK_LEN,
                temperature = TEMP,
                cfg_scale   = CFG_SCALE,
                remasking   = REMASKING
            )

            # 3) ë””ì½”ë”© (prompt ì´í›„ë§Œ)
            gen_txt = unified_eeg_text_tokenizer.llada_text_tokenizer.batch_decode(
                out_ids[:, prompt_len:], skip_special_tokens=True
            )[0].strip()
    
            # 4) ì¦‰ì‹œ CSVë¡œ flush (ì¤‘ê°„ì— ëŠê²¨ë„ ë°ì´í„° ë³´ì¡´)
            with open(csv_path, "a", newline="", encoding="utf-8") as f:
                csv.writer(f).writerow([ref_text, gen_txt])

            all_ref.append(ref_text)
            all_hyp.append(gen_txt)

    # â”€â”€ ìµœì¢… ì§€í‘œ ê³„ì‚° & ì €ì¥ â”€â”€
    bleu  = calculate_bleu_scores(all_ref, all_hyp)
    rouge = calculate_rouge_scores(all_ref, all_hyp)
    wer   = calculate_wer(all_ref, all_hyp)
    metrics = {"BLEU": bleu, "ROUGE": rouge, "WER": wer}
    with open(metrics_path, "w", encoding="utf-8") as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)

    print("ğŸ“  finished! metrics saved â†’", metrics_path)