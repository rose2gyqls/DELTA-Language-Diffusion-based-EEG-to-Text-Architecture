{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1bc03b-9de1-4342-a5fb-cd6774796d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig, AutoTokenizer # BitsAndBytesConfig Ï∂îÍ∞Ä\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training,PeftModel ,TaskType # peft Í¥ÄÎ†® Î™®Îìà Ï∂îÍ∞Ä\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "import itertools\n",
    "import gc\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "import jiwer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any, List\n",
    "import math\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321243b7-90c1-4837-96e2-9a312d8a5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: 1\n",
      "Device name: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "print(\"Available devices:\", torch.cuda.device_count())  # 1Í∞úÎßå Î≥¥Ïó¨Ïïº Ï†ïÏÉÅ\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))    # Ïã§Ï†úÎ°ú GPU 1Î≤à Ïù¥Î¶ÑÏù¥ ÎÇòÏò¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f059b57-021f-4827-802b-8a43ee348fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î™®Îì† ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò ÏãúÎìúÍ∞Ä 42Î°ú Í≥†Ï†ïÎêòÏóàÏäµÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "def set_seeds(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # Ïó¨Îü¨ GPU ÏÇ¨Ïö© Ïãú\n",
    "        # CUDA Ïó∞ÏÇ∞Ïùò Í≤∞Ï†ïÎ°†Ï†Å Ïã§Ìñâ ÏÑ§Ï†ï (ÏÑ±Îä•Ïóê ÏïΩÍ∞Ñ ÏòÅÌñ• Ï§Ñ Ïàò ÏûàÏùå)\n",
    "        # torch.backends.cudnn.deterministic = True\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(SEED)\n",
    "print(f\"Î™®Îì† ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò ÏãúÎìúÍ∞Ä {SEED}Î°ú Í≥†Ï†ïÎêòÏóàÏäµÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e68e65-79f1-4c0e-9ac6-7aac3f969163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_dir = \"/home/work/skku/hyo/hyo/dataset/sentence.parquet\"):\n",
    "        df = pd.read_parquet(data_dir)\n",
    "        eeg_vecs = df[\"eeg\"].to_numpy()\n",
    "\n",
    "        arr = np.stack(eeg_vecs).astype(np.float32)\n",
    "        arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mu, std = arr.mean(0, keepdims=True), arr.std(0, keepdims=True)+1e-8\n",
    "        self.eeg_arr = (arr - mu) / std      # Ï†ïÍ∑úÌôî\n",
    "        self.text_arr = df[\"text\"].to_numpy() # ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞\n",
    "        self.data = list(zip(torch.tensor(self.eeg_arr), self.text_arr))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class ConvEEGEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    840-dim Î≤°ÌÑ∞Î•º 1√ó840 ÏãúÌÄÄÏä§Î°ú Î≥¥Í≥† Conv1D Îëê Ï∏µÏúºÎ°ú Ïû†Ïû¨ÌëúÌòÑ ÏÉùÏÑ±\n",
    "    Ï∂úÎ†•ÏùÄ (B, latent_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=840, latent_dim=128, hidden=256):\n",
    "        super().__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(hidden, latent_dim, kernel_size=3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)   # Í∏∏Ïù¥ 840 ‚Üí 1 Î°ú ÏïïÏ∂ï\n",
    "\n",
    "    def forward(self, x):           # x: (B, feat)\n",
    "        x = x.unsqueeze(1)          # (B, 1, 840)\n",
    "        z = self.conv_stack(x)      # (B, latent_dim, 840)\n",
    "        z = self.pool(z).squeeze(-1)  # (B, latent_dim)\n",
    "        return z\n",
    "\n",
    "class RVQ(nn.Module):\n",
    "    def __init__(self, num_quantizers, num_embeddings, embedding_dim, commitment_cost=0.25):\n",
    "        super().__init__()\n",
    "        self.num_quantizers = num_quantizers # ÏΩîÎìúÎ∂ÅÏùò Í∞úÏàò (n_q)\n",
    "        self.num_embeddings = num_embeddings # Í∞Å ÏΩîÎìúÎ∂Å ÎÇ¥ ÏûÑÎ≤†Îî©(ÏΩîÎìúÏõåÎìú) Í∞úÏàò (n_emb, Ïñ¥Ìúò ÌÅ¨Í∏∞)\n",
    "        self.embedding_dim = embedding_dim   # Í∞Å ÏûÑÎ≤†Îî©Ïùò Ï∞®Ïõê (D, latent_dimÍ≥º ÎèôÏùº)\n",
    "        self.commitment_cost = commitment_cost # VQ ÏÜêÏã§ Í≥ÑÏÇ∞ Ïãú ÏÇ¨Ïö©ÎêòÎäî ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
    "\n",
    "        # num_quantizers Í∞úÏàòÎßåÌÅºÏùò ÏΩîÎìúÎ∂Å(nn.Embedding Î†àÏù¥Ïñ¥)ÏùÑ Î¶¨Ïä§Ìä∏Î°ú Í∞ÄÏßê\n",
    "        self.codebooks = nn.ModuleList([\n",
    "            nn.Embedding(self.num_embeddings, self.embedding_dim) for _ in range(self.num_quantizers)\n",
    "        ])\n",
    "        # ÏΩîÎìúÎ∂Å Í∞ÄÏ§ëÏπò Ï¥àÍ∏∞Ìôî (ÏÑ†ÌÉù ÏÇ¨Ìï≠Ïù¥ÏßÄÎßå ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÏàòÌñâ)\n",
    "        for i, codebook in enumerate(self.codebooks):\n",
    "            nn.init.uniform_(codebook.weight, -1.0 / self.num_embeddings, 1.0 / self.num_embeddings)\n",
    "\n",
    "    def forward(self, z_e): # ÏûÖÎ†• z_eÏùò Î™®Ïñë: (B, L, D), Ïó¨Í∏∞ÏÑú L=1, D=embedding_dim\n",
    "        B, L, D = z_e.shape\n",
    "        z_e_flat = z_e.reshape(-1, D) # (B*L, D) Î™®ÏñëÏúºÎ°ú ÌéºÏπ® (Ïó¨Í∏∞ÏÑúÎäî (B, D)ÏôÄ ÎèôÏùº)\n",
    "\n",
    "        all_quantized_stages = [] # Í∞Å ÏΩîÎìúÎ∂ÅÏóêÏÑú ÏñëÏûêÌôîÎêú Î≤°ÌÑ∞Îì§ÏùÑ Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏\n",
    "        all_indices = []          # Í∞Å ÏΩîÎìúÎ∂ÅÏóêÏÑú ÏÑ†ÌÉùÎêú Ïù∏Îç±Ïä§Îì§ÏùÑ Ï†ÄÏû•Ìï† Î¶¨Ïä§Ìä∏\n",
    "        residual = z_e_flat       # Ï≤´ Î≤àÏß∏ ÏΩîÎìúÎ∂ÅÏóê ÏûÖÎ†•Îê† ÏûîÏ∞® (Ï¥àÍ∏∞ÏóêÎäî z_e_flat Ï†ÑÏ≤¥)\n",
    "\n",
    "        # num_quantizers ÎßåÌÅº Î∞òÎ≥µ (Í∞Å ÏΩîÎìúÎ∂ÅÏóê ÎåÄÌï¥ ÏàúÏ∞®Ï†ÅÏúºÎ°ú Ï≤òÎ¶¨)\n",
    "        for i in range(self.num_quantizers):\n",
    "            codebook = self.codebooks[i] # ÌòÑÏû¨ ÏÇ¨Ïö©Ìï† ÏΩîÎìúÎ∂Å\n",
    "\n",
    "            # ÌòÑÏû¨ ÏûîÏ∞®(residual)ÏôÄ ÌòÑÏû¨ ÏΩîÎìúÎ∂ÅÏùò Î™®Îì† ÏûÑÎ≤†Îî© Í∞ÑÏùò Ïú†ÌÅ¥Î¶¨Îìú Í±∞Î¶¨ Ï†úÍ≥± Í≥ÑÏÇ∞\n",
    "            # distances Î™®Ïñë: (B*L, num_embeddings)\n",
    "            distances = torch.sum(residual**2, dim=1, keepdim=True) \\\n",
    "                        - 2 * torch.matmul(residual, codebook.weight.t()) \\\n",
    "                        + torch.sum(codebook.weight**2, dim=1, keepdim=True).t()\n",
    "\n",
    "            # Í∞ÄÏû• Í∞ÄÍπåÏö¥ ÏûÑÎ≤†Îî©Ïùò Ïù∏Îç±Ïä§ Ï∞æÍ∏∞\n",
    "            # current_indices Î™®Ïñë: (B*L)\n",
    "            current_indices = torch.argmin(distances, dim=1)\n",
    "            all_indices.append(current_indices) # ÌòÑÏû¨ ÏΩîÎìúÎ∂ÅÏùò Ïù∏Îç±Ïä§ Ï†ÄÏû•\n",
    "\n",
    "            # ÏÑ†ÌÉùÎêú Ïù∏Îç±Ïä§Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÏñëÏûêÌôîÎêú Î≤°ÌÑ∞(ÏΩîÎìúÏõåÎìú) Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "            # quantized_vector Î™®Ïñë: (B*L, D)\n",
    "            quantized_vector = codebook(current_indices)\n",
    "            # ÏõêÎûò Î™®Ïñë (B, L, D)Î°ú Î≥µÏõêÌïòÏó¨ Ï†ÄÏû• (Ïó¨Í∏∞ÏÑúÎäî (B, 1, D))\n",
    "            all_quantized_stages.append(quantized_vector.reshape(B, L, D))\n",
    "\n",
    "            # Îã§Ïùå ÏΩîÎìúÎ∂ÅÏúºÎ°ú ÎÑòÍ∏∏ ÏûîÏ∞® Í≥ÑÏÇ∞\n",
    "            # Ï§ëÏöî: quantized_vectorÏóêÏÑú Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌùêÎ¶ÑÏùÑ ÎÅäÍ∏∞ ÏúÑÌï¥ .detach() ÏÇ¨Ïö©\n",
    "            residual = residual - quantized_vector.detach()\n",
    "\n",
    "        # Î™®Îì† ÏΩîÎìúÎ∂ÅÏóêÏÑú ÎÇòÏò® ÏñëÏûêÌôîÎêú Î≤°ÌÑ∞Îì§ÏùÑ Ìï©ÏÇ∞ (EEGTran ÎÖºÎ¨∏ Figure 2 Ï∞∏Ï°∞)\n",
    "        # final_quantized_output Î™®Ïñë: (B, L, D)\n",
    "        final_quantized_output = torch.stack(all_quantized_stages, dim=0).sum(dim=0)\n",
    "\n",
    "        # ÏàòÏßëÎêú Ïù∏Îç±Ïä§Îì§ÏùÑ (B, L, num_quantizers) ÌòïÌÉúÎ°ú ÏåìÏùå\n",
    "        # stacked_indices Î™®Ïñë: (B, L, n_q) (Ïó¨Í∏∞ÏÑúÎäî (B, 1, n_q))\n",
    "        stacked_indices = torch.stack(all_indices, dim=1).reshape(B, L, self.num_quantizers)\n",
    "\n",
    "        # ÏµúÏ¢Ö Î∞òÌôòÍ∞í: Ìï©ÏÇ∞Îêú ÏñëÏûêÌôî Î≤°ÌÑ∞, ÏåìÏù∏ Ïù∏Îç±Ïä§ ÏãúÌÄÄÏä§, VQ ÏÜêÏã§\n",
    "        # RVQTokenizerÏùò forwardÏóêÏÑúÎäî Ïù¥ Ï§ë Ï≤´ Îëê Í∞úÎ•º zq, indicesÎ°ú Î∞õÍ≤å Îê©ÎãàÎã§.\n",
    "        return final_quantized_output, stacked_indices\n",
    "\n",
    "class RVQTokenizer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 feat=840,\n",
    "                 latent=128,  # 1024->2048\n",
    "                 n_q=12,\n",
    "                 n_emb=512,\n",
    "                 hidden=256,\n",
    "                 TOKENIZER_CHECKPOINT_PATH = \"/home/work/skku/hyo/hyo/model/rvq_best_model_sen_512.pt\"\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.n_q = n_q\n",
    "        self.n_emb = n_emb\n",
    "        # Ïã§Ï†ú ConvEEGEncoderÏôÄ RVQ Î™®ÎìàÏù¥ Ïó¨Í∏∞Ïóê ÏôÄÏïº Ìï®\n",
    "        self.enc = ConvEEGEncoder(feat, latent, hidden)\n",
    "        self.rvq = RVQ(num_quantizers=n_q, num_embeddings=n_emb, embedding_dim=latent)\n",
    "\n",
    "        checkpoint = torch.load(TOKENIZER_CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "        self.enc.load_state_dict(checkpoint[\"encoder\"])\n",
    "        for i, cb_weight_tensor in enumerate(checkpoint[\"codebooks\"]):\n",
    "            self.rvq.codebooks[i].weight.data = cb_weight_tensor\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x): # x: (B, 840)\n",
    "        z = self.enc(x)\n",
    "        quantized_vector, token_indices = self.rvq(z.unsqueeze(1)) # vq_lossÎäî Î¨¥Ïãú\n",
    "        zq = quantized_vector\n",
    "        indices = token_indices # Î™®Ïñë (B, 1, n_q)\n",
    "        # ÎßåÏïΩ LLaDA ÏûÖÎ†•Ïö©ÏúºÎ°ú (B, n_q) Î™®ÏñëÏùò Ïù∏Îç±Ïä§Î•º ÏõêÌïúÎã§Î©¥ squeeze(1) ÌïÑÏöî\n",
    "        # return zq, indices.squeeze(1)\n",
    "        return zq, indices # ÌòÑÏû¨ pasted_content.txtÏùò Ï£ºÏÑùÍ≥º ÎßûÏ∂îÎ†§Î©¥ Ïù¥ÎåÄÎ°ú\n",
    "\n",
    "class UnifiedEEGTextTokenizer:\n",
    "    def __init__(self,\n",
    "                rvq_tokenizer_instance,\n",
    "                llada_text_tokenizer_instance,\n",
    "                max_seq_length,\n",
    "                v_text_original,\n",
    "                eeg_token_length,\n",
    "                ):\n",
    "\n",
    "        self.rvq_tokenizer = rvq_tokenizer_instance\n",
    "        self.llada_text_tokenizer = llada_text_tokenizer_instance\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.v_text_original = v_text_original\n",
    "        self.eeg_token_length = eeg_token_length\n",
    "\n",
    "\n",
    "        self.bos_token_id = torch.tensor([self.llada_text_tokenizer.bos_token_id], dtype=torch.long, device=config.system.DEVICE)\n",
    "        self.eos_token_id = torch.tensor([self.llada_text_tokenizer.eos_token_id], dtype=torch.long, device=config.system.DEVICE)\n",
    "        self.pad_token_id = self.llada_text_tokenizer.pad_token_id if self.llada_text_tokenizer.pad_token_id is not None else self.llada_text_tokenizer.eos_token_id\n",
    "\n",
    "        self.user_prompt_intro_ids = self.llada_text_tokenizer.encode(\n",
    "                \"<start_id>user<end_id>\\n\",\n",
    "                add_special_tokens=False,\n",
    "                return_tensors=\"pt\"\n",
    "            ).squeeze(0).to(config.system.DEVICE)\n",
    "\n",
    "        self.assistant_prompt_intro_ids = self.llada_text_tokenizer.encode(\n",
    "                \"<eot_id><start_id>assistant<end_id>\\n\",\n",
    "                add_special_tokens=False,\n",
    "                return_tensors=\"pt\"\n",
    "            ).squeeze(0).to(config.system.DEVICE)\n",
    "\n",
    "        print(f\"Unified Tokenizer Initialized:\")\n",
    "        print(f\"  BOS ID: {self.bos_token_id.item()}\")\n",
    "        print(f\"  EOS ID: {self.eos_token_id.item()}\")\n",
    "        print(f\"  PAD ID: {self.pad_token_id}\")\n",
    "        print(f\"  User Prompt Intro IDs ({len(self.user_prompt_intro_ids)} tokens): {self.user_prompt_intro_ids.tolist()}\")\n",
    "        print(f\"  Assistant Prompt Intro IDs ({len(self.assistant_prompt_intro_ids)} tokens): {self.assistant_prompt_intro_ids.tolist()}\")\n",
    "\n",
    "    def process_single_sample(self, eeg_tensor, assistant_response_text):\n",
    "        eeg_tensor = eeg_tensor.to(config.system.DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, local_eeg_indices = self.rvq_tokenizer(eeg_tensor.unsqueeze(0))\n",
    "        local_eeg_indices = local_eeg_indices.squeeze(0).squeeze(0)\n",
    "        if local_eeg_indices.ndim == 0: local_eeg_indices = local_eeg_indices.unsqueeze(0)\n",
    "        global_eeg_ids = (local_eeg_indices + self.v_text_original).to(config.system.DEVICE)\n",
    "\n",
    "        len_fixed_tokens = (\n",
    "            len(self.bos_token_id) +\n",
    "            len(self.user_prompt_intro_ids) +\n",
    "            self.eeg_token_length +\n",
    "            len(self.assistant_prompt_intro_ids) +\n",
    "            len(self.eos_token_id)\n",
    "        )\n",
    "        max_assistant_text_len = self.max_seq_length - len_fixed_tokens\n",
    "\n",
    "        if max_assistant_text_len <= 0:\n",
    "            # print(f\"Warning: Not enough space for assistant text. Max assistant length: {max_assistant_text_len}. Truncating or using empty.\")\n",
    "            assistant_response_text = \"\"\n",
    "            max_assistant_text_len = 1 # Ensure at least 1 token space if possible, or handle error\n",
    "\n",
    "        tokenized_assistant_text = self.llada_text_tokenizer(\n",
    "            assistant_response_text,\n",
    "            max_length=max(1, max_assistant_text_len),\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        assistant_text_ids = tokenized_assistant_text.input_ids.squeeze(0).to(config.system.DEVICE)\n",
    "\n",
    "        input_ids_list = [\n",
    "            self.bos_token_id,\n",
    "            self.user_prompt_intro_ids,\n",
    "            global_eeg_ids, # This is the {USER_CONTENT}\n",
    "            self.assistant_prompt_intro_ids,\n",
    "            assistant_text_ids, # This is the {ASSISTANT_CONTENT}\n",
    "            self.eos_token_id\n",
    "        ]\n",
    "        input_ids = torch.cat(input_ids_list, dim=0)\n",
    "\n",
    "        prompt_len = (\n",
    "                len(self.bos_token_id) +\n",
    "                len(self.user_prompt_intro_ids) +\n",
    "                len(global_eeg_ids) +\n",
    "                len(self.assistant_prompt_intro_ids)\n",
    "            )\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:prompt_len] = -100 # ÌîÑÎ°¨ÌîÑÌä∏ Î∂ÄÎ∂ÑÏùÄ ÏÜêÏã§ Í≥ÑÏÇ∞ÏóêÏÑú Ï†úÏô∏\n",
    "        # EOS ÌÜ†ÌÅ∞ÎèÑ ÏòàÏ∏° ÎåÄÏÉÅÏù¥ ÏïÑÎãàÎùºÎ©¥ -100 Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏúºÎÇò, Î≥¥ÌÜµÏùÄ ÏòàÏ∏° ÎåÄÏÉÅÏóê Ìè¨Ìï®.\n",
    "\n",
    "        current_len = len(input_ids)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        if current_len < self.max_seq_length:\n",
    "            padding_len = self.max_seq_length - current_len\n",
    "            pad_values = torch.full((padding_len,), self.pad_token_id, dtype=torch.long, device=config.system.DEVICE)\n",
    "            input_ids = torch.cat([input_ids, pad_values], dim=0)\n",
    "            labels = torch.cat([labels, torch.full((padding_len,), -100, dtype=torch.long, device=config.system.DEVICE)], dim=0)\n",
    "            attention_mask = torch.cat([attention_mask, torch.zeros((padding_len,), dtype=torch.long, device=config.system.DEVICE)], dim=0)\n",
    "        elif current_len > self.max_seq_length:\n",
    "            input_ids = input_ids[:self.max_seq_length]\n",
    "            labels = labels[:self.max_seq_length]\n",
    "            attention_mask = attention_mask[:self.max_seq_length]\n",
    "            # prompt_len = min(prompt_len, self.max_seq_length) # ÏûòÎ¶∞ Í≤ΩÏö∞ ÌîÑÎ°¨ÌîÑÌä∏ Í∏∏Ïù¥ÎèÑ Ï°∞Ï†ïÎê† Ïàò ÏûàÏùå\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"prompt_lengths\": torch.tensor(prompt_len, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def build_chat_template_prompt(self, eeg_tensor):\n",
    "        \"\"\"\n",
    "        EEG ÏûÖÎ†•ÎßåÏúºÎ°ú inferenceÏö© prompt Íµ¨ÏÑ±.\n",
    "        assistant_response_text ÏóÜÏù¥ ÏÉùÏÑ±.\n",
    "        \"\"\"\n",
    "        eeg_tensor = eeg_tensor.to(config.system.DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, local_eeg_indices = self.rvq_tokenizer(eeg_tensor.unsqueeze(0))  # (1, L)\n",
    "        local_eeg_indices = local_eeg_indices.squeeze(0).squeeze(0)\n",
    "        if local_eeg_indices.ndim == 0:\n",
    "            local_eeg_indices = local_eeg_indices.unsqueeze(0)\n",
    "\n",
    "        global_eeg_ids = (local_eeg_indices + self.v_text_original).to(config.system.DEVICE)\n",
    "\n",
    "        # ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ± (text ÏóÜÏù¥)\n",
    "        input_ids_list = [\n",
    "            self.bos_token_id,\n",
    "            self.user_prompt_intro_ids,\n",
    "            global_eeg_ids,  # USER_CONTENT (EEG ÌÜ†ÌÅ∞)\n",
    "            self.assistant_prompt_intro_ids  # ASSISTANT_CONTENT ÌÖúÌîåÎ¶øÍπåÏßÄÎßå\n",
    "        ]\n",
    "        input_ids = torch.cat(input_ids_list, dim=0)\n",
    "\n",
    "        prompt_len = len(input_ids)\n",
    "\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.unsqueeze(0),          # (1, T)\n",
    "            \"attention_mask\": attention_mask.unsqueeze(0),# (1, T)\n",
    "            \"prompt_len\": prompt_len                      # int\n",
    "        }\n",
    "\n",
    "class DataCollatorForEEGTextSFT:\n",
    "    def __init__(self, unified_tokenizer_instance):\n",
    "        self.unified_tokenizer = unified_tokenizer_instance\n",
    "\n",
    "    def __call__(self, batch_of_samples):\n",
    "        processed_samples = []\n",
    "        for eeg_tensor, assistant_response_text in batch_of_samples:\n",
    "            if eeg_tensor is None or assistant_response_text is None:\n",
    "                continue\n",
    "            processed_samples.append(self.unified_tokenizer.process_single_sample(eeg_tensor, assistant_response_text))\n",
    "\n",
    "        if not processed_samples:\n",
    "            # print(\"Warning: Collator received no valid samples to batch.\")\n",
    "            # Îπà ÌÖêÏÑúÎ•º Î∞òÌôòÌïòÍ±∞ÎÇò NoneÏùÑ Î∞òÌôòÌïòÏó¨ ÌïôÏäµ Î£®ÌîÑÏóêÏÑú Ï≤òÎ¶¨\n",
    "            return None\n",
    "\n",
    "        batched_input_ids = torch.stack([s[\"input_ids\"] for s in processed_samples])\n",
    "        batched_attention_mask = torch.stack([s[\"attention_mask\"] for s in processed_samples])\n",
    "        batched_labels = torch.stack([s[\"labels\"] for s in processed_samples])\n",
    "        batched_prompt_lengths = torch.stack([s[\"prompt_lengths\"] for s in processed_samples])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": batched_input_ids,\n",
    "            \"attention_mask\": batched_attention_mask,\n",
    "            \"labels\": batched_labels,\n",
    "            \"prompt_lengths\": batched_prompt_lengths\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35636db7-c796-4615-a774-323ee90baa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_for_evaluation(batch):\n",
    "    eeg_data_list = [item[0] for item in batch] # ÏõêÎ≥∏ EEG Îç∞Ïù¥ÌÑ∞ Î¶¨Ïä§Ìä∏\n",
    "    reference_texts_list = [item[1] for item in batch] # Ï∞∏Ï°∞ ÌÖçÏä§Ìä∏ Î¶¨Ïä§Ìä∏\n",
    "\n",
    "    # EEG Îç∞Ïù¥ÌÑ∞Îäî Î∞∞Ïπò ÎÇ¥ÏóêÏÑú Ìå®Îî© ÏóÜÏù¥ Î¶¨Ïä§Ìä∏ ÌòïÌÉúÎ°ú Ïú†ÏßÄÌïòÍ±∞ÎÇò,\n",
    "    # ÎßåÏïΩ Î™®Îì† EEG Îç∞Ïù¥ÌÑ∞Ïùò Í∏∏Ïù¥Í∞Ä Í∞ôÎã§Î©¥ torch.stackÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "    # Ïó¨Í∏∞ÏÑúÎäî Î¶¨Ïä§Ìä∏ ÌòïÌÉúÎ°ú Î∞òÌôòÌïòÍ≥†, ÏÉùÏÑ± Î£®ÌîÑÏóêÏÑú Í∞úÎ≥Ñ Ï≤òÎ¶¨ÌïúÎã§Í≥† Í∞ÄÏ†ïÌï©ÎãàÎã§.\n",
    "    return {\n",
    "        \"batched_eeg_data\": eeg_data_list, \n",
    "        \"batched_reference_texts\": reference_texts_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dbe808c-59ca-4888-912d-696d0e1569fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu_scores(references, hypotheses):\n",
    "    \"\"\"\n",
    "    references: List[List[List[str]]]  # Í∞Å ÏÉòÌîåÎßàÎã§ Ïó¨Îü¨ Î†àÌçºÎü∞Ïä§ Î¨∏Ïû•(ÌÜ†ÌÅ∞ÌôîÎêú Î¶¨Ïä§Ìä∏)\n",
    "    hypotheses: List[List[str]]        # Í∞Å ÏÉòÌîåÎßàÎã§ ÏÉùÏÑ±Îêú Î¨∏Ïû•(ÌÜ†ÌÅ∞ÌôîÎêú Î¶¨Ïä§Ìä∏)\n",
    "    \n",
    "    Î∞òÌôò: {\n",
    "        \"BLEU-1\": float,\n",
    "        \"BLEU-2\": float,\n",
    "        \"BLEU-3\": float,\n",
    "        \"BLEU-4\": float\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Ïä§Î¨¥Îî© Ìï®Ïàò\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    \n",
    "    # Ï∏°Ï†ïÌï† BLEU weight ÏÑ§Ï†ï\n",
    "    weight_dict = {\n",
    "        \"BLEU-1\": (1.0, 0.0, 0.0, 0.0),\n",
    "        \"BLEU-2\": (0.5, 0.5, 0.0, 0.0),\n",
    "        \"BLEU-3\": (1/3, 1/3, 1/3, 0.0),\n",
    "        \"BLEU-4\": (0.25, 0.25, 0.25, 0.25),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, weights in weight_dict.items():\n",
    "        scores = []\n",
    "        for refs, hyp in zip(references, hypotheses):\n",
    "            # Îπà Î¨∏ÏûêÏó¥Ïù¥ÎÇò Î†àÌçºÎü∞Ïä§Í∞Ä ÏóÜÏúºÎ©¥ 0Ï†ê Ï≤òÎ¶¨\n",
    "            if not refs or not refs[0] or not hyp:\n",
    "                scores.append(0.0)\n",
    "                continue\n",
    "            score = sentence_bleu(\n",
    "                refs,\n",
    "                hyp,\n",
    "                weights=weights,\n",
    "                smoothing_function=smooth_fn\n",
    "            )\n",
    "            scores.append(score)\n",
    "        # ÏÉòÌîåÎ≥Ñ Ï†êÏàò ÌèâÍ∑†\n",
    "        results[name] = float(np.mean(scores)) if scores else 0.0\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2afcb9a6-0ae5-4920-8876-9f22cd2fc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(references, hypotheses):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    all_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        if not ref or not hyp:\n",
    "            # Îπà Î¨∏ÏûêÏó¥ Ï≤òÎ¶¨ ÎòêÎäî ÌäπÏ†ï Í∞í Ìï†Îãπ\n",
    "            for key in all_scores:\n",
    "                all_scores[key].append(0.0) # f-measure Í∏∞Ï§Ä 0Ï†êÏúºÎ°ú Ï≤òÎ¶¨\n",
    "            continue\n",
    "        scores = scorer.score(ref, hyp)\n",
    "        all_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        all_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        all_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    avg_scores = {key: np.mean(values) if values else 0.0 for key, values in all_scores.items()}\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962a9165-7075-4c60-8f16-099869e80774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Ï£ºÏñ¥ÏßÑ Ï†ïÎãµÍ≥º ÏòàÏ∏°Ïóê ÎåÄÌï¥ WERÏùÑ Í≥ÑÏÇ∞Ìï©ÎãàÎã§.\n",
    "    references: list of strings (Í∞Å ÏÉòÌîåÏóê ÎåÄÌïú Ï†ïÎãµ Î¨∏Ïû• Î¶¨Ïä§Ìä∏)\n",
    "    hypotheses: list of strings (Í∞Å ÏÉòÌîåÏóê ÎåÄÌïú ÏòàÏ∏° Î¨∏Ïû• Î¶¨Ïä§Ìä∏)\n",
    "    \"\"\"\n",
    "    if not references or not hypotheses or len(references) != len(hypotheses):\n",
    "        return 1.0 # ÏûòÎ™ªÎêú ÏûÖÎ†• Ï≤òÎ¶¨, WERÏùÄ ÎÇÆÏùÑÏàòÎ°ù Ï¢ãÏúºÎØÄÎ°ú 1.0 (100% Ïò§Î•ò) Î∞òÌôò\n",
    "\n",
    "    # jiwer.compute_measuresÎäî Ï†ÑÏ≤¥ Î¶¨Ïä§Ìä∏Ïóê ÎåÄÌï¥ ÏßëÍ≥ÑÎêú Í≤∞Í≥ºÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n",
    "    # Í∞úÎ≥Ñ Î¨∏Ïû•Ïóê ÎåÄÌïú WERÏùÑ Í≥ÑÏÇ∞ÌïòÍ≥† ÌèâÍ∑†ÎÇ¥Îäî Í≤ÉÎ≥¥Îã§, Ï†ÑÏ≤¥ ÎßêÎ≠âÏπòÏóê ÎåÄÌïú WERÏùÑ Í≥ÑÏÇ∞ÌïòÎäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏûÖÎãàÎã§.\n",
    "    measures = jiwer.compute_measures(references, hypotheses)\n",
    "    return measures['wer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b79a7a9-4a3f-4afa-ae2f-0134c3dd6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gumbel_noise(logits, temperature):\n",
    "    '''\n",
    "    The Gumbel max is a method for sampling categorical distributions.\n",
    "    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n",
    "    Thus, we use float64.\n",
    "    '''\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (- torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    '''\n",
    "    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n",
    "    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n",
    "    the expected number of tokens transitioned at each step should be consistent.\n",
    "\n",
    "    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n",
    "    '''\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "\n",
    "    return num_transfer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "632781e7-9222-4a31-a169-14ee78b041e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ torch.no_grad()\n",
    "def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=126336):\n",
    "    '''\n",
    "    Args:\n",
    "        model: Mask predictor.\n",
    "        prompt: A tensor of shape (1, L).\n",
    "        steps: Sampling steps, less than or equal to gen_length.\n",
    "        gen_length: Generated answer length.\n",
    "        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.\n",
    "        temperature: Categorical distribution sampling temperature.\n",
    "        cfg_scale: Unsupervised classifier-free guidance scale.\n",
    "        remasking: Remasking strategy. 'low_confidence' or 'random'.\n",
    "        mask_id: The toke id of [MASK] is 126336.\n",
    "    '''\n",
    "    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "\n",
    "    assert steps % num_blocks == 0\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    for num_block in range(num_blocks):\n",
    "        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\n",
    "        for i in range(steps):\n",
    "            mask_index = (x == mask_id)\n",
    "            if cfg_scale > 0.:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_).logits\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l\n",
    "\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits.to(torch.float64), dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(remasking)\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\n",
    "                transfer_index[j, select_index] = True\n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6259a975-c4d8-41e5-89ec-d95ea60711ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_eeg_llada(\n",
    "        base_ckpt: str = \"GSAI-ML/LLaDA-8B-Base\",\n",
    "        comp_dir: Path = Path(\n",
    "            \"/home/work/skku/hyo/hyo/checkpoints_v2/best_finetuned_model_components\"  # wte.pth, lm_head.pth, adapters/\n",
    "        ),\n",
    "        rvq_n_emb: int = 512,\n",
    "        device: str = \"cuda\",\n",
    "        load_in_4bit: bool = True,\n",
    "):\n",
    "    # --- ‚ë† base LLaDA (4-bit optional) ------------------------------\n",
    "    bnb_cfg = (BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n",
    "        if load_in_4bit else None)\n",
    "\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_ckpt, quantization_config=bnb_cfg, torch_dtype=\"auto\",\n",
    "        trust_remote_code=True).to(device)\n",
    "\n",
    "    # --- ‚ë° vocab resize --------------------------------------------\n",
    "    v_txt = base.config.vocab_size\n",
    "    base.resize_token_embeddings(v_txt + rvq_n_emb + 1)   # +1 = EEG-MASK\n",
    "\n",
    "    # --- ‚ë¢ embedding / lm_head Î≥µÏõê -------------------------------\n",
    "    wte_path = comp_dir / \"wte.pth\"\n",
    "    lm_path = comp_dir / \"lm_head.pth\"\n",
    "\n",
    "    llada_core = getattr(base, \"model\", base)          # 1Ï∞®\n",
    "    llada_core = getattr(llada_core, \"model\", llada_core)  # 2Ï∞®\n",
    "    #   3-2. transformer Ìï∏Îì§\n",
    "    transformer = getattr(llada_core, \"transformer\", llada_core)\n",
    "\n",
    "    if wte_path.exists():\n",
    "        base.get_input_embeddings().load_state_dict(torch.load(wte_path, map_location=\"cpu\"))\n",
    "        print(\"‚úî wte restored\")\n",
    "\n",
    "    if lm_path.exists():\n",
    "        transformer.ff_out.load_state_dict(torch.load(lm_path, map_location=\"cpu\"))\n",
    "        print(\"‚úî lm_head (ff_out) restored\")\n",
    "\n",
    "    # --- ‚ë£ LoRA attach (trainable=False Î°ú inference) --------------\n",
    "    adapter_dir = comp_dir / \"adapters\"\n",
    "    if adapter_dir.exists():\n",
    "        model = PeftModel.from_pretrained(base, adapter_dir, is_trainable=False)\n",
    "        print(\"‚úî LoRA adapter loaded\")\n",
    "    else:\n",
    "        model = base\n",
    "        print(\"‚ö† LoRA adapter not found, base model only\")\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e86a26e4-6e94-4979-a301-76c03eed4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemConfig(BaseModel):\n",
    "    SEED: int = 42\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    NUM_WORKERS: int = 0\n",
    "\n",
    "class PathsConfig(BaseModel):\n",
    "    DATASET_PATH: str = \"/home/work/skku/hyo/hyo/dataset/sentence.parquet\"\n",
    "    TOKENIZER_CHECKPOINT_PATH: str = \"/home/work/skku/hyo/hyo/model/rvq_best_model_sen_512.pt\"\n",
    "    MODEL_SAVE_DIR: str = \"./saved_models\"\n",
    "    BEST_MODEL_FILENAME: str = \"eeg_llada_sft_best_model.pth\"\n",
    "    # LLADA_LOSS_FUNCTION_PATH: str = \"/home/ubuntu/llada_loss_function.py\" # ÌïÑÏöîÏãú Ï∂îÍ∞Ä\n",
    "    # MODIFIED_TRAINING_LOOPS_PATH: str = \"/home/ubuntu/modified_training_loops.py\" # ÌïÑÏöîÏãú Ï∂îÍ∞Ä\n",
    "\n",
    "class EEGEncoderConfig(BaseModel):\n",
    "    INPUT_DIM: int = 840\n",
    "    LATENT_DIM: int = 128 # RVQÏùò embedding_dimÍ≥º ÏùºÏπòÌï¥Ïïº Ìï®\n",
    "    HIDDEN_DIM: int = 256\n",
    "\n",
    "class RVQConfig(BaseModel):\n",
    "    NUM_QUANTIZERS: int = 12 # RVQTokenizerÏùò n_q, UnifiedEEGTextTokenizerÏùò eeg_token_lengthÏôÄ ÏùºÏπò\n",
    "    NUM_EMBEDDINGS: int = 512 # RVQTokenizerÏùò n_emb\n",
    "    EMBEDDING_DIM: int = 128 # EEGEncoderConfigÏùò LATENT_DIMÍ≥º ÏùºÏπò\n",
    "    COMMITMENT_COST: float = 0.25\n",
    "\n",
    "class TokenizerConfig(BaseModel):\n",
    "    # RVQTokenizer ÎÇ¥Î∂Ä ÌååÎùºÎØ∏ÌÑ∞ (EEGEncoderConfig, RVQConfig Í∞íÏúºÎ°ú ÎåÄÏ≤¥ Í∞ÄÎä•)\n",
    "    # UnifiedEEGTextTokenizer ÌååÎùºÎØ∏ÌÑ∞\n",
    "    MAX_SEQ_LENGTH: int = 1024 # LLaDA Î™®Îç∏Ïùò ÏµúÎåÄ Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Í≥†Î†§\n",
    "    V_TEXT_ORIGINAL: int = 32000 # LLaMA ÌÖçÏä§Ìä∏ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÏùò Ïñ¥Ìúò ÌÅ¨Í∏∞ (LLaDA-8B Í∏∞Ï§Ä)\n",
    "    # EEG_TOKEN_LENGTH: int = 12 # RVQConfig.NUM_QUANTIZERS ÏôÄ ÎèôÏùº\n",
    "    LLM_MODEL_NAME: str = \"GSAI-ML/LLaDA-8B-Base\" # LLM ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìúÏö©\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    LLM_MODEL_NAME: str = \"GSAI-ML/LLaDA-8B-Base\"\n",
    "    USE_QLORA: bool = True\n",
    "    LORA_R: int = 16\n",
    "    LORA_ALPHA: int = 32\n",
    "    LORA_DROPOUT: float = 0.05\n",
    "    LORA_BIAS: str = \"none\"\n",
    "    # LLADA_MASK_TOKEN_ID: Optional[int] = None # ÎèôÏ†ÅÏúºÎ°ú ÏÑ§Ï†ïÎê† Ïàò ÏûàÏùå (Ïñ¥ÌúòÌÅ¨Í∏∞ + 1)\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    BATCH_SIZE: int = 4 # GPU Î©îÎ™®Î¶¨Ïóê Îî∞Îùº Ï°∞Ï†à\n",
    "    NUM_EPOCHS: int = 10\n",
    "    START_EPOCH: int = 0\n",
    "    LEARNING_RATE: float = 1e-4\n",
    "    GRADIENT_ACCUMULATION_STEPS: int = 4 # BATCH_SIZE * GRAD_ACCUM = Ïã§Ï†ú Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "    MAX_GRAD_NORM: float = 1.0\n",
    "    # SCHEDULER: Optional[str] = None # Ïòà: \"StepLR\"\n",
    "    TRAIN_LOG_INTERVAL: int = 1\n",
    "    PATIENCE_EARLY_STOPPING: int = 3 # 0Ïù¥Î©¥ ÎπÑÌôúÏÑ±Ìôî\n",
    "\n",
    "class GenerationConfig(BaseModel):\n",
    "    RUN_TEST_LOOP_EACH_EPOCH: bool = False\n",
    "    USE_LLADA_SAMPLING_FOR_GENERATION: bool = True\n",
    "    MAX_GEN_TOKENS: int = 64\n",
    "    NUM_SAMPLING_STEPS_GEN: int = 10\n",
    "    REMASKING_STRATEGY_GEN: str = \"low_confidence\"\n",
    "    REMASKING_RATIO_GEN: float = 0.25\n",
    "    TEMPERATURE_GEN: float = 0.7\n",
    "    TOP_K_GEN: int = 50\n",
    "    TOP_P_GEN: float = 0.9\n",
    "    HF_NUM_BEAMS_GEN: int = 1\n",
    "    # HF_MAX_LENGTH_GEN: Optional[int] = None # ÎèôÏ†ÅÏúºÎ°ú ÏÑ§Ï†ï (ÏûÖÎ†•Í∏∏Ïù¥ + MAX_GEN_TOKENS)\n",
    "\n",
    "class ExperimentConfig(BaseModel):\n",
    "    system: SystemConfig = Field(default_factory=SystemConfig)\n",
    "    paths: PathsConfig = Field(default_factory=PathsConfig)\n",
    "    eeg_encoder: EEGEncoderConfig = Field(default_factory=EEGEncoderConfig)\n",
    "    rvq: RVQConfig = Field(default_factory=RVQConfig)\n",
    "    tokenizer: TokenizerConfig = Field(default_factory=TokenizerConfig)\n",
    "    model: ModelConfig = Field(default_factory=ModelConfig)\n",
    "    training: TrainingConfig = Field(default_factory=TrainingConfig)\n",
    "    generation: GenerationConfig = Field(default_factory=GenerationConfig)\n",
    "\n",
    "    # LLADA_MASK_TOKEN_IDÎäî ÎèôÏ†ÅÏúºÎ°ú ÏÑ§Ï†ïÎê† Ïàò ÏûàÏúºÎØÄÎ°ú, Ï¥àÍ∏∞Ìôî ÌõÑ ÏÑ§Ï†ïÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•\n",
    "    # Ïòà: config.model.LLADA_MASK_TOKEN_ID = tokenizer.llada_text_tokenizer.vocab_size + 1\n",
    "\n",
    "config = ExperimentConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "235940ff-13a2-419d-9dcb-2f2bb5f0bb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=123 environment variable detected; loading libbitsandbytes_cuda123.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938b06085a944e20be32583df8e0bfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî wte restored\n",
      "‚úî lm_head (ff_out) restored\n",
      "‚úî LoRA adapter loaded\n"
     ]
    }
   ],
   "source": [
    "comp_dir = Path(\"/home/work/skku/hyo/hyo/checkpoints_v2/best_finetuned_model_components\")\n",
    "model = load_finetuned_eeg_llada(\n",
    "            base_ckpt=\"GSAI-ML/LLaDA-8B-Base\",\n",
    "            comp_dir=comp_dir,\n",
    "            rvq_n_emb=512,\n",
    "            device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05947423-82dc-4d73-9750-b9b1c408839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Tokenizer Initialized:\n",
      "  BOS ID: 126080\n",
      "  EOS ID: 126081\n",
      "  PAD ID: 126081\n",
      "  User Prompt Intro IDs (10 tokens): [27, 7351, 2983, 29, 3840, 27, 486, 2983, 29, 198]\n",
      "  Assistant Prompt Intro IDs (15 tokens): [27, 68, 335, 2983, 3583, 7351, 2983, 29, 598, 10450, 27, 486, 2983, 29, 198]\n"
     ]
    }
   ],
   "source": [
    "llada_txt_tokenizer = AutoTokenizer.from_pretrained(config.model.LLM_MODEL_NAME)\n",
    "rvq_eeg_tokenizer = RVQTokenizer()\n",
    "rvq_eeg_tokenizer = rvq_eeg_tokenizer.to(config.system.DEVICE)\n",
    "rvq_eeg_tokenizer.eval() # Ï∂îÎ°† Î™®ÎìúÎ°ú ÏÑ§Ï†ï\n",
    "\n",
    "v_original = llada_txt_tokenizer.vocab_size\n",
    "eeg_seq_len = 12 # RVQ_N_Q\n",
    "model_max_len = 512\n",
    "\n",
    "unified_eeg_text_tokenizer = UnifiedEEGTextTokenizer(\n",
    "    rvq_tokenizer_instance=rvq_eeg_tokenizer,\n",
    "    llada_text_tokenizer_instance=llada_txt_tokenizer,\n",
    "    max_seq_length=model_max_len,\n",
    "    v_text_original=v_original,\n",
    "    eeg_token_length=eeg_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f9608b1-d850-48a6-bf91-796c746497d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞: 25616\n",
      "ÌïôÏäµ ÏÑ∏Ìä∏ ÌÅ¨Í∏∞: 20492 (Ï†ÑÏ≤¥Ïùò 80.00%)\n",
      "Í≤ÄÏ¶ù ÏÑ∏Ìä∏ ÌÅ¨Í∏∞: 2562 (Ï†ÑÏ≤¥Ïùò 10.00%)\n",
      "ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏ ÌÅ¨Í∏∞: 2562 (Ï†ÑÏ≤¥Ïùò 10.00%)\n",
      "\n",
      "ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Î°úÎçî Î∞∞Ïπò Ïàò: 5123\n",
      "Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Î°úÎçî Î∞∞Ïπò Ïàò: 641\n",
      "ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î°úÎçî Î∞∞Ïπò Ïàò: 641\n",
      "\n",
      "Îç∞Ïù¥ÌÑ∞ Î°úÎî© Î∞è Î∂ÑÌï† (config Í∏∞Î∞ò) ÏôÑÎ£å.\n"
     ]
    }
   ],
   "source": [
    "# 1. EEGDataset Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± (config ÏÇ¨Ïö©)\n",
    "# EEGDataset ÌÅ¥ÎûòÏä§ Ï†ïÏùòÎäî Ïù¥ÎØ∏ ÎÖ∏Ìä∏Î∂ÅÏóê ÏûàÎã§Í≥† Í∞ÄÏ†ïÌï©ÎãàÎã§.\n",
    "eeg_dataset = EEGDataset(data_dir=config.paths.DATASET_PATH)\n",
    "eeg_dataset = eeg_dataset # ÌÖåÏä§Ìä∏Ïö©\n",
    "num_total_samples = len(eeg_dataset)\n",
    "indices = list(range(num_total_samples))\n",
    "\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÌï† (config ÏÇ¨Ïö©)\n",
    "# Ï∞∏Í≥†: test_size Í∞íÎì§(ÌòÑÏû¨ 0.2 Î∞è 0.5)ÎèÑ config Í∞ùÏ≤¥Ïóê Ï∂îÍ∞ÄÌïòÏó¨ Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "# Ïòà: config.training.TRAIN_VAL_SPLIT_RATIO, config.training.VAL_TEST_SPLIT_RATIO\n",
    "train_indices, temp_test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2, # Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Ï§ë 20%Î•º (Í≤ÄÏ¶ù+ÌÖåÏä§Ìä∏)Ïö©ÏúºÎ°ú Î∂ÑÎ¶¨\n",
    "    random_state=config.system.SEED, # configÏóêÏÑú SEED Í∞í ÏÇ¨Ïö©\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_indices, test_indices = train_test_split(\n",
    "    temp_test_indices,\n",
    "    test_size=0.5, # (Í≤ÄÏ¶ù+ÌÖåÏä§Ìä∏)Ïö© Îç∞Ïù¥ÌÑ∞ Ï§ë 50%Î•º ÌÖåÏä§Ìä∏Ïö©ÏúºÎ°ú Î∂ÑÎ¶¨ (Ï¶â, Ï†ÑÏ≤¥Ïùò 10%)\n",
    "    random_state=config.system.SEED, # configÏóêÏÑú SEED Í∞í ÏÇ¨Ïö©\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Í∞Å Subset ÏÉùÏÑ±\n",
    "train_dataset = Subset(eeg_dataset, train_indices)\n",
    "val_dataset = Subset(eeg_dataset, val_indices)\n",
    "test_dataset = Subset(eeg_dataset, test_indices)\n",
    "\n",
    "print(f\"Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞: {num_total_samples}\")\n",
    "print(f\"ÌïôÏäµ ÏÑ∏Ìä∏ ÌÅ¨Í∏∞: {len(train_dataset)} (Ï†ÑÏ≤¥Ïùò {len(train_dataset)/num_total_samples:.2%})\")\n",
    "print(f\"Í≤ÄÏ¶ù ÏÑ∏Ìä∏ ÌÅ¨Í∏∞: {len(val_dataset)} (Ï†ÑÏ≤¥Ïùò {len(val_dataset)/num_total_samples:.2%})\")\n",
    "print(f\"ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏ ÌÅ¨Í∏∞: {len(test_dataset)} (Ï†ÑÏ≤¥Ïùò {len(test_dataset)/num_total_samples:.2%})\")\n",
    "\n",
    "# DataCollatorForEEGTextSFT Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± (Ïù¥Ï†ÑÏóê data_collator Î°ú Ï†ïÏùòÎêòÏóàÎã§Í≥† Í∞ÄÏ†ï)\n",
    "data_collator = DataCollatorForEEGTextSFT(unified_eeg_text_tokenizer)\n",
    "\n",
    "# 3. DataLoader ÏÉùÏÑ± (config ÏÇ¨Ïö©)\n",
    "# ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.training.BATCH_SIZE, # configÏóêÏÑú BATCH_SIZE Í∞í ÏÇ¨Ïö©\n",
    "    collate_fn=collate_fn_for_evaluation,\n",
    "    shuffle=True,\n",
    "    num_workers=config.system.NUM_WORKERS, # configÏóêÏÑú NUM_WORKERS Í∞í ÏÇ¨Ïö©\n",
    "    pin_memory= False\n",
    ")\n",
    "\n",
    "# Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.training.BATCH_SIZE, # configÏóêÏÑú BATCH_SIZE Í∞í ÏÇ¨Ïö©\n",
    "    collate_fn=collate_fn_for_evaluation,\n",
    "    shuffle=False,\n",
    "    num_workers=config.system.NUM_WORKERS, # configÏóêÏÑú NUM_WORKERS Í∞í ÏÇ¨Ïö©\n",
    "    pin_memory= False\n",
    ")\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.training.BATCH_SIZE, # configÏóêÏÑú BATCH_SIZE Í∞í ÏÇ¨Ïö©\n",
    "    collate_fn=collate_fn_for_evaluation,\n",
    "    shuffle=False,\n",
    "    num_workers=config.system.NUM_WORKERS, # configÏóêÏÑú NUM_WORKERS Í∞í ÏÇ¨Ïö©\n",
    "    pin_memory= False\n",
    ")\n",
    "\n",
    "print(f\"\\nÌïôÏäµ Îç∞Ïù¥ÌÑ∞Î°úÎçî Î∞∞Ïπò Ïàò: {len(train_dataloader)}\")\n",
    "print(f\"Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Î°úÎçî Î∞∞Ïπò Ïàò: {len(val_dataloader)}\")\n",
    "print(f\"ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î°úÎçî Î∞∞Ïπò Ïàò: {len(test_dataloader)}\")\n",
    "\n",
    "\n",
    "print(\"\\nÎç∞Ïù¥ÌÑ∞ Î°úÎî© Î∞è Î∂ÑÌï† (config Í∏∞Î∞ò) ÏôÑÎ£å.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae724fb6-b6e1-46de-b089-f2006bf67bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch, numpy as np\n",
    "import json\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÏÉòÌîåÎßÅ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "GEN_STEPS   = 64    # diffusion Ïä§ÌÖù\n",
    "GEN_LEN     = 128        # ÏÉùÏÑ± ÌÜ†ÌÅ∞ Ïàò\n",
    "BLOCK_LEN   = 64\n",
    "TEMP        = 0.3\n",
    "CFG_SCALE   = 0.9\n",
    "REMASKING   = \"low_confidence\"\n",
    "\n",
    "device = next(model.parameters()).device        # Î™®Îç∏Ïù¥ Ïò¨ÎùºÍ∞Ñ GPU/CPU\n",
    "\n",
    "model.eval()\n",
    "all_hyp, all_ref = [], []\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Í≤∞Í≥º Ï†ÄÏû•Ïö© ‚îÄ‚îÄ‚îÄ\n",
    "out_dir      = Path(\"/home/work/skku/hyo/hyo/generate\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "csv_path     = out_dir / \"eeg_gen_results.csv\"\n",
    "metrics_path = out_dir / \"final_metrics.json\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"üîÆ  Generating from EEG\"):\n",
    "        eeg_list = batch[\"batched_eeg_data\"]            # list[Tensor(840)]\n",
    "        ref_list = batch[\"batched_reference_texts\"]     # list[str]\n",
    "\n",
    "        for eeg_tensor, ref_text in zip(eeg_list, ref_list):\n",
    "            # 1) EEG  ‚ûú  RVQ  ‚ûú  prompt\n",
    "            prompt_pack = unified_eeg_text_tokenizer.build_chat_template_prompt(eeg_tensor)\n",
    "            prompt_ids  = prompt_pack[\"input_ids\"].to(device)          # (1, T)\n",
    "            prompt_len  = prompt_pack[\"prompt_len\"]\n",
    "            # EEG ÌÜ†ÌÅ∞ id Í∞Ä ÏÑúÎ°ú Îã§Î•∏ÏßÄ\n",
    "            # print(prompt_ids[0, :20])          # Îã§Î•∏ EEG sample ÏóêÏÑú Í∞íÏù¥ Îã¨ÎùºÏïº Ìï®\n",
    "\n",
    "            # # Ïñ¥ÎåëÌÑ∞ Î°úÎìúÎêêÎäîÏßÄ\n",
    "            # print(model.peft_config.keys())    # LoRA ÏÑ§Ï†ï dict Í∞Ä ÎπÑÏñ¥ ÏûàÏúºÎ©¥ Ïñ¥ÎåëÌÑ∞ ÎØ∏Ï†ÅÏö©\n",
    "\n",
    "            # 2) LLaDA diffusion sampling\n",
    "            out_ids = generate(\n",
    "                model,                                  # ‚Üê Î∞îÎ°ú model Ï†ÑÎã¨\n",
    "                prompt = prompt_ids,         # (T,)\n",
    "                steps  = GEN_STEPS,\n",
    "                gen_length = GEN_LEN,\n",
    "                block_length = BLOCK_LEN,\n",
    "                temperature = TEMP,\n",
    "                cfg_scale   = CFG_SCALE,\n",
    "                remasking   = REMASKING\n",
    "            )\n",
    "\n",
    "            # 3) ÎîîÏΩîÎî© (prompt Ïù¥ÌõÑÎßå)\n",
    "            gen_txt = unified_eeg_text_tokenizer.llada_text_tokenizer.batch_decode(\n",
    "                out_ids[:, prompt_len:], skip_special_tokens=True\n",
    "            )[0].strip()\n",
    "    \n",
    "            # 4) Ï¶âÏãú CSVÎ°ú flush (Ï§ëÍ∞ÑÏóê ÎÅäÍ≤®ÎèÑ Îç∞Ïù¥ÌÑ∞ Î≥¥Ï°¥)\n",
    "            with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                csv.writer(f).writerow([ref_text, gen_txt])\n",
    "\n",
    "            all_ref.append(ref_text)\n",
    "            all_hyp.append(gen_txt)\n",
    "\n",
    "    # ‚îÄ‚îÄ ÏµúÏ¢Ö ÏßÄÌëú Í≥ÑÏÇ∞ & Ï†ÄÏû• ‚îÄ‚îÄ\n",
    "    bleu  = calculate_bleu_scores(all_ref, all_hyp)\n",
    "    rouge = calculate_rouge_scores(all_ref, all_hyp)\n",
    "    wer   = calculate_wer(all_ref, all_hyp)\n",
    "    metrics = {\"BLEU\": bleu, \"ROUGE\": rouge, \"WER\": wer}\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"üìù  finished! metrics saved ‚Üí\", metrics_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyo",
   "language": "python",
   "name": "hyo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
